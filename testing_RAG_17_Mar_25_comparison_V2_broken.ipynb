{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc549157-03fd-47f4-b7ee-e1d78e3d5ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stored document text embeddings in FAISS database.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "import fitz  \n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "### DOCUMENT PROCESSING ###\n",
    "\n",
    "# Load text embedding model\n",
    "text_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a given PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text(\"text\") + \"\\n\"\n",
    "    return text.strip()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Enhance retrieval by applying TF-IDF weighting.\"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
    "    X = vectorizer.fit_transform([text]).toarray()\n",
    "    return \" \".join(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Directory setup\n",
    "base_dir = \"business_flowcharts\"\n",
    "pdf_dir = os.path.join(base_dir, \"documents\")\n",
    "\n",
    "# Read and process PDFs\n",
    "pdf_texts = {}\n",
    "pdf_filenames = sorted([f for f in os.listdir(pdf_dir) if f.endswith(\".pdf\")])\n",
    "\n",
    "pdf_embeddings = []\n",
    "for file in pdf_filenames:\n",
    "    pdf_path = os.path.join(pdf_dir, file)\n",
    "    raw_text = extract_text_from_pdf(pdf_path)\n",
    "    processed_text = preprocess_text(raw_text)\n",
    "    pdf_texts[file] = raw_text\n",
    "    embedding = text_model.encode(processed_text)\n",
    "    pdf_embeddings.append(embedding)\n",
    "\n",
    "# Convert to FAISS-compatible format\n",
    "pdf_embeddings = np.array(pdf_embeddings, dtype=\"float32\")\n",
    "pdf_embeddings /= np.linalg.norm(pdf_embeddings, axis=1, keepdims=True)  # Normalize (IMPORTANT)\n",
    "\n",
    "# Create FAISS index\n",
    "text_index = faiss.IndexFlatL2(pdf_embeddings.shape[1])\n",
    "text_index.add(pdf_embeddings)\n",
    "\n",
    "print(\"‚úÖ Stored document text embeddings in FAISS database.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "234472c5-6981-489f-afda-da8f1d784f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stored flowchart image embeddings in FAISS database.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "### FLOWCHART IMG PROCESSING ###\n",
    "\n",
    "# Load CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"Preprocess an image dynamically while maintaining aspect ratio.\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Resize while keeping aspect ratio\n",
    "    aspect_ratio = image.width / image.height\n",
    "    if aspect_ratio > 1:\n",
    "        new_width = 224\n",
    "        new_height = int(224 / aspect_ratio)\n",
    "    else:\n",
    "        new_height = 224\n",
    "        new_width = int(224 * aspect_ratio)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((new_height, new_width)),  # Maintain aspect ratio\n",
    "        transforms.Pad((0, 0, 224 - new_width, 224 - new_height), fill=(255, 255, 255)),  # Pad with white\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.481, 0.457, 0.408], std=[0.268, 0.261, 0.275]),  # CLIP normalization (ALSO IMPORATNT)\n",
    "    ])\n",
    "\n",
    "    return transform(image).unsqueeze(0)\n",
    "\n",
    "def get_image_embedding(image_path):\n",
    "    \"\"\"Generate an embedding for a flowchart image using CLIP.\"\"\"\n",
    "    image_tensor = preprocess_image(image_path).to(device)\n",
    "    with torch.no_grad():\n",
    "        embedding = clip_model.encode_image(image_tensor).cpu().numpy()\n",
    "    return embedding.flatten()\n",
    "\n",
    "# Directory setup\n",
    "image_dir = os.path.join(base_dir, \"flowcharts\")\n",
    "image_filenames = sorted([f for f in os.listdir(image_dir) if f.endswith(\".png\")])\n",
    "\n",
    "# Process images\n",
    "image_embeddings = [get_image_embedding(os.path.join(image_dir, file)) for file in image_filenames]\n",
    "image_embeddings = np.array(image_embeddings, dtype=\"float32\")\n",
    "image_embeddings /= np.linalg.norm(image_embeddings, axis=1, keepdims=True)  # Normalize\n",
    "\n",
    "# Create FAISS index\n",
    "image_index = faiss.IndexFlatL2(image_embeddings.shape[1])\n",
    "image_index.add(image_embeddings)\n",
    "\n",
    "print(\"‚úÖ Stored flowchart image embeddings in FAISS database.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31fcba5a-5490-47e1-9685-7e66bd084120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import re\n",
    "import fitz  \n",
    "\n",
    "def extract_text_from_image(image_path):\n",
    "    \"\"\"Extract text from a given flowchart image using OCR.\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    extracted_text = pytesseract.image_to_string(image)\n",
    "    return extracted_text.strip()\n",
    "\n",
    "def get_text_embedding(text):\n",
    "    \"\"\"Generate an embedding for extracted text using Sentence-BERT.\"\"\"\n",
    "    return text_model.encode(text)\n",
    "\n",
    "# def extract_text_from_user_pdf(pdf_path):\n",
    "#     \"\"\"Extract text from a user-uploaded PDF file.\"\"\"\n",
    "#     text = \"\"\n",
    "#     with fitz.open(pdf_path) as doc:\n",
    "#         for page in doc:\n",
    "#             text += page.get_text(\"text\") + \"\\n\"\n",
    "#     return text.strip()\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2285b3dc-6fcf-4600-9620-b7540d81ec4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Retrieval function is ready.\n"
     ]
    }
   ],
   "source": [
    "def get_query_image_embedding(image_path):\n",
    "    \"\"\"Generate a normalized embedding for a query image using CLIP.\"\"\"\n",
    "    image_tensor = preprocess_image(image_path).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embedding = clip_model.encode_image(image_tensor).cpu().numpy()\n",
    "    \n",
    "    return embedding.flatten() / np.linalg.norm(embedding)  # Normalize\n",
    "\n",
    "\n",
    "def retrieve_relevant_data(query, flowchart_img=None, top_k=2):\n",
    "    \"\"\"Retrieve the most relevant documents & images for the query using FAISS.\"\"\"\n",
    "\n",
    "    # Convert query to text embedding\n",
    "    query_embedding = text_model.encode(query).reshape(1, -1)\n",
    "    query_embedding /= np.linalg.norm(query_embedding)  # Normalize\n",
    "\n",
    "    # Search FAISS text database (Retrieve relevant PDFs)\n",
    "    _, text_results = text_index.search(query_embedding, top_k)\n",
    "    retrieved_pdfs = [pdf_filenames[idx] for idx in text_results[0]]\n",
    "\n",
    "    # Process flowchart image if provided\n",
    "    if flowchart_img:\n",
    "        extracted_text = extract_text_from_image(flowchart_img)\n",
    "        text_embedding = get_text_embedding(extracted_text)\n",
    "\n",
    "        # Use extracted text to retrieve relevant documents\n",
    "        _, text_results = text_index.search(text_embedding.reshape(1, -1), top_k)\n",
    "        retrieved_pdfs = [pdf_filenames[idx] for idx in text_results[0]]\n",
    "\n",
    "        # Generate CLIP image embedding as well\n",
    "        image_query_embedding = get_query_image_embedding(flowchart_img)\n",
    "    else:\n",
    "        # Convert text query into CLIP text embedding\n",
    "        text_tokenized = clip.tokenize([query]).to(device)\n",
    "        with torch.no_grad():\n",
    "            image_query_embedding = clip_model.encode_text(text_tokenized).cpu().numpy()\n",
    "\n",
    "    image_query_embedding = image_query_embedding.reshape(1, -1)\n",
    "    image_query_embedding /= np.linalg.norm(image_query_embedding)\n",
    "\n",
    "    # Search FAISS image database (Retrieve relevant flowcharts)\n",
    "    _, image_results = image_index.search(image_query_embedding, top_k)\n",
    "    retrieved_images = [image_filenames[idx] for idx in image_results[0]]\n",
    "\n",
    "    return retrieved_pdfs, retrieved_images\n",
    "\n",
    "print(\"‚úÖ Retrieval function is ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd22c25b-b1a2-459b-926b-60adfc08d7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972d2245ed0c43c89b81fe5c42992d98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n",
    "\n",
    "# Load Qwen model\n",
    "qwen_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\", \n",
    "    torch_dtype=torch.bfloat16,  \n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "qwen_processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "031ac69c-2195-4f38-86df-414c6e47d450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def debug_query_qwen_with_rag(query, flowchart_img=None, top_k=2):\n",
    "#     \"\"\"DEBUG MODE - Retrieve relevant flowchart data (text & images) and query Qwen for an AI-generated response.\"\"\"\n",
    "    \n",
    "#     # Retrieve relevant PDFs (text) & Flowcharts (images)\n",
    "#     retrieved_pdfs, retrieved_images = retrieve_relevant_data(query, flowchart_img, top_k)\n",
    "    \n",
    "#     # Extract text from retrieved PDFs\n",
    "#     context = \"\\n\".join([pdf_texts[pdf] for pdf in retrieved_pdfs])\n",
    "\n",
    "#     # Use retrieved flowchart image if `flowchart_img` is None\n",
    "#     image_path = flowchart_img if flowchart_img else os.path.join(image_dir, retrieved_images[0])\n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "#     # Define the user message (Injecting retrieved context)\n",
    "#     messages = [\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": [\n",
    "#                 {\"type\": \"image\", \"image\": image},\n",
    "#                 {\"type\": \"text\", \"text\": f\"Attached is document providing context. Based on the provided image and textual information, please analyze the content and generate a response that accurately addresses the user's inquiry.\\n\\nContext:\\n{context}\\n\\nQuery: {query}\"},\n",
    "#             ],\n",
    "#         }\n",
    "#     ]\n",
    "    \n",
    "#     # Format input for Qwen\n",
    "#     text = qwen_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "#     inputs = qwen_processor(\n",
    "#         text=[text],\n",
    "#         images=[image],\n",
    "#         padding=True,\n",
    "#         return_tensors=\"pt\",\n",
    "#     ).to(qwen_model.device)\n",
    "    \n",
    "#     # Generate response\n",
    "#     with torch.no_grad():\n",
    "#         output_ids = qwen_model.generate(**inputs, max_new_tokens=512)\n",
    "    \n",
    "#     # Decode response\n",
    "#     response_text = \"========\\n\\n\".join(qwen_processor.batch_decode(output_ids, skip_special_tokens=False))\n",
    "    \n",
    "#     return response_text, retrieved_pdfs, retrieved_images\n",
    "\n",
    "# print(\"‚úÖ DEBUG Qwen RAG system is ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cee3aa1f-4093-4a85-bf0b-08e159d6f64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Multi-Image Qwen RAG system is ready.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from PIL import Image\n",
    "\n",
    "def query_qwen_with_rag(query, flowchart_imgs=None, top_k=2):\n",
    "    \"\"\"Retrieve relevant flowchart data (text & images) and query Qwen for an AI-generated response.\"\"\"\n",
    "\n",
    "    retrieved_pdfs = []\n",
    "    retrieved_images = []\n",
    "    extracted_texts = []\n",
    "    image_objects = []\n",
    "    image_embeddings = []\n",
    "\n",
    "    # Process each image if multiple images are provided\n",
    "    if flowchart_imgs:\n",
    "        for img_path in flowchart_imgs:\n",
    "            extracted_text = extract_text_from_image(img_path)  # OCR text extraction\n",
    "            extracted_texts.append(extracted_text)\n",
    "\n",
    "            # Generate text embedding from extracted OCR text\n",
    "            text_embedding = get_text_embedding(extracted_text)\n",
    "            _, text_results = text_index.search(text_embedding.reshape(1, -1), top_k)\n",
    "            retrieved_pdfs.extend([pdf_filenames[idx] for idx in text_results[0]])\n",
    "\n",
    "            # Generate CLIP image embeddings\n",
    "            image_embedding = get_query_image_embedding(img_path)\n",
    "            image_embeddings.append(image_embedding)\n",
    "\n",
    "            # Load image for Qwen\n",
    "            image_objects.append(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "    else:\n",
    "        extracted_texts.append(\"\")\n",
    "\n",
    "    # Aggregate extracted texts from multiple images\n",
    "    extracted_text_context = \"\\n\".join(extracted_texts)\n",
    "\n",
    "    # Aggregate multiple image embeddings (mean vector)\n",
    "    if image_embeddings:\n",
    "        aggregated_image_embedding = np.mean(image_embeddings, axis=0)\n",
    "        aggregated_image_embedding /= np.linalg.norm(aggregated_image_embedding)  # Normalize\n",
    "\n",
    "        # Retrieve relevant flowcharts from FAISS\n",
    "        _, image_results = image_index.search(aggregated_image_embedding.reshape(1, -1), top_k)\n",
    "        retrieved_images = [image_filenames[idx] for idx in image_results[0]]\n",
    "\n",
    "    # Retrieve documents based on text query\n",
    "    query_embedding = text_model.encode(query).reshape(1, -1)\n",
    "    query_embedding /= np.linalg.norm(query_embedding)\n",
    "\n",
    "    _, text_results = text_index.search(query_embedding, top_k)\n",
    "    retrieved_pdfs.extend([pdf_filenames[idx] for idx in text_results[0]])\n",
    "\n",
    "    # Deduplicate retrieved PDFs\n",
    "    retrieved_pdfs = list(set(retrieved_pdfs))\n",
    "\n",
    "    # Define the user message (Injecting retrieved context)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                *[{\"type\": \"image\", \"image\": img} for img in image_objects],  # Pass all images\n",
    "                # {\"type\": \"text\", \"text\": f\"Given the attached flowcharts, analyze their content.\\n\\nExtracted Flowchart Texts:\\n{extracted_text_context}\\n\\nContext from Documents:\\n{retrieved_pdfs}\\n\\nQuery: {query}\"},\n",
    "                 {\"type\": \"text\", \"text\": f\"Given the attached document, which contains essential context, please extract the relevant information from the provided content before generating any response. Your task is to prioritize and reference the details from the document, specifically focusing on the key facts and structures outlined. If additional knowledge is required, ensure that the output draws from the context directly to prevent any inaccuracies or hallucinations. Do not introduce external or unverified information unless it is explicitly supported by the provided document here or by the user. Answer only in English language.\\n\\nExtracted Flowchart Texts:\\n{extracted_text_context}\\n\\nContext:\\n{retrieved_pdfs}\\n\\nQuery: {query}\"},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Format input for Qwen\n",
    "    text = qwen_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = qwen_processor(\n",
    "        text=[text],\n",
    "        images=image_objects,  # Pass multiple images\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(qwen_model.device)\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        output_ids = qwen_model.generate(**inputs, max_new_tokens=800)\n",
    "    \n",
    "    # Decode response\n",
    "    response_text = qwen_processor.batch_decode(output_ids, skip_special_tokens=False)[0]\n",
    "\n",
    "    # Extract Assistant's Response Only\n",
    "    match = re.search(r\"assistant\\s*\\n(.*)\", response_text, re.DOTALL)\n",
    "    cleaned_response = match.group(1).strip() if match else response_text.strip()\n",
    "\n",
    "    return cleaned_response, retrieved_pdfs, retrieved_images\n",
    "\n",
    "print(\"‚úÖ Multi-Image Qwen RAG system is ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ae4f055-8f6e-48fb-b117-c4ae3962c608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Qwen's Response (With RAG):\n",
      "The flowchart you have is titled \"Quality Control & Inspection.\" Here is an explanation of each step:\n",
      "\n",
      "1. **Start**: The process begins with starting the quality control and inspection procedure.\n",
      "2. **Select Sample for Inspection**: A sample of the product or material is chosen for inspection to ensure that the entire batch meets the quality standards.\n",
      "3. **Conduct Quality Tests**: The selected sample undergoes various tests to evaluate its compliance with the established quality standards.\n",
      "4. **Meets Quality Standards?**: After conducting the tests, the question arises whether the sample meets the quality standards set forth.\n",
      "   - **Yes**: If the sample meets the quality standards, the next step is to **Approve for Distribution**.\n",
      "   - **No**: If the sample does not meet the quality standards, the process moves to **Reject & Investigate**.\n",
      "5. **Reject & Investigate**: If the sample is rejected, further investigation is conducted to determine why it did not meet the standards.\n",
      "6. **Rework Defective Units**: Based on the investigation, defective units may need to be reworked to meet the quality standards.\n",
      "7. **Inspection Completed**: Once the appropriate actions (approval for distribution or rework) are taken, the inspection process is considered complete.\n",
      "\n",
      "This flowchart outlines the systematic approach to ensuring that products or materials meet quality standards through a series of inspections and corrective actions when necessary.<|im_end|>\n",
      "üîç Retrieved Documents: ['17_Medical_Diagnosis.pdf', '18_Medication_Prescription.pdf', '14_Quality_Control.pdf', '5.pdf']\n",
      "üñºÔ∏è Retrieved Flowcharts: ['14_quality_control.png', '6_incident_management.png']\n",
      "‚è≥ Execution Time: 5.25 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Define the test query and multiple flowchart images\n",
    "# user_query = \"Provided here is an incident management flowchart. The first one is in the databse, the second one is an improved one I'm working on. How many nodes are on each flowcharts? List it all out\"\n",
    "# user_query = \"Can you compare these flowcharts in detail? What are the difference between the first and the second one. I'm working on the second one to give more detail.\"\n",
    "user_query = \"What flowchart do I have here? Can you explain all the steps?\"\n",
    "flowchart_imgs = [    \n",
    "    \"business_flowcharts/flowcharts/14_quality_control.png\"\n",
    "    # \"business_flowcharts/flowcharts/6_incident_management.png\",\n",
    "    # \"6_incident_management_v2.png\",\n",
    "]\n",
    "# user_pdf = [\n",
    "#     \"6_Incident_Management_V2.pdf\n",
    "# ]\n",
    "\n",
    "# \"business_flowcharts/flowcharts/6_incident_management.png\",\n",
    "\n",
    "# Measure execution time\n",
    "start_time = time.time()\n",
    "\n",
    "# Query Qwen with RAG\n",
    "qwen_response_rag, retrieved_pdfs, retrieved_images = query_qwen_with_rag(user_query, flowchart_imgs)\n",
    "\n",
    "# Calculate total time taken\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nü§ñ Qwen's Response (With RAG):\\n{qwen_response_rag}\")\n",
    "print(f\"üîç Retrieved Documents: {retrieved_pdfs}\")\n",
    "print(f\"üñºÔ∏è Retrieved Flowcharts: {retrieved_images}\")\n",
    "# Add user uploaded images here\n",
    "# Add user uploaded files here\n",
    "print(f\"‚è≥ Execution Time: {execution_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0cbc62a-af1c-47bb-858d-d5f0b2c71c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def debug_flowchart_inputs(flowchart_imgs):\n",
    "#     \"\"\"Display and log flowchart images along with extracted OCR text.\"\"\"\n",
    "    \n",
    "#     fig, axes = plt.subplots(1, len(flowchart_imgs), figsize=(10 * len(flowchart_imgs), 5))\n",
    "    \n",
    "#     if len(flowchart_imgs) == 1:\n",
    "#         axes = [axes]  # Ensure iterable for single image\n",
    "    \n",
    "#     extracted_texts = []\n",
    "    \n",
    "#     for i, img_path in enumerate(flowchart_imgs):\n",
    "#         extracted_text = extract_text_from_image(img_path)\n",
    "#         extracted_texts.append(extracted_text)\n",
    "#         print(f\"\\nüîç Extracted Text from {img_path}:\\n{extracted_text}\\n\")\n",
    "        \n",
    "#         img = Image.open(img_path)\n",
    "#         axes[i].imshow(img)\n",
    "#         axes[i].set_title(f\"Flowchart {i+1}\")\n",
    "#         axes[i].axis(\"off\")\n",
    "\n",
    "#     plt.show()\n",
    "    \n",
    "#     return extracted_texts\n",
    "\n",
    "# debug_flowchart_inputs(flowchart_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26fda206-ca13-4497-9313-462900efb1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test saved documents in FAISS database\n",
    "# def view_stored_pdfs():\n",
    "#     \"\"\"Display all stored PDFs and their extracted text.\"\"\"\n",
    "#     for filename, text in pdf_texts.items():\n",
    "#         print(f\"üìÑ PDF: {filename}\\n\")\n",
    "#         print(f\"Extracted Content:\\n{text[:1000]}\")  # Show first 1000 characters\n",
    "#         print(\"=\"*80)\n",
    "\n",
    "# view_stored_pdfs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "766448a9-e470-49d0-9eff-3adef9c37473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Test saved images in FAISS database\n",
    "\n",
    "# def view_stored_flowcharts():\n",
    "#     \"\"\"Display all stored flowchart images and their filenames.\"\"\"\n",
    "#     for filename in image_filenames:\n",
    "#         image_path = os.path.join(image_dir, filename)\n",
    "#         image = Image.open(image_path)\n",
    "        \n",
    "#         plt.figure(figsize=(5, 5))\n",
    "#         plt.imshow(image)\n",
    "#         plt.axis(\"off\")\n",
    "#         plt.title(f\"üñºÔ∏è Flowchart: {filename}\")\n",
    "#         plt.show()\n",
    "\n",
    "# view_stored_flowcharts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9e5fcdf-793f-41ba-a300-ff851d41b6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_pdf_embedding(index=12):\n",
    "#     \"\"\"Check stored text embedding by retrieving the closest match for a given PDF.\"\"\"\n",
    "#     query_embedding = pdf_embeddings[index].reshape(1, -1)\n",
    "#     _, retrieved_indices = text_index.search(query_embedding, 1)\n",
    "    \n",
    "#     original_pdf = pdf_filenames[index]\n",
    "#     matched_pdf = pdf_filenames[retrieved_indices[0][0]]\n",
    "    \n",
    "#     print(f\"üìÑ Original PDF: {original_pdf}\")\n",
    "#     print(f\"üîç Closest Match: {matched_pdf}\")\n",
    "#     print(f\"Similarity Score: {np.dot(pdf_embeddings[index], pdf_embeddings[retrieved_indices[0][0]])}\")\n",
    "    \n",
    "# check_pdf_embedding()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b23561cd-0ea7-427f-8c03-41c278131caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def check_image_embedding(index=3):\n",
    "#     \"\"\"Check stored image embedding by retrieving the closest match for a given flowchart.\"\"\"\n",
    "#     query_embedding = image_embeddings[index].reshape(1, -1)\n",
    "#     _, retrieved_indices = image_index.search(query_embedding, 1)\n",
    "    \n",
    "#     original_image = image_filenames[index]\n",
    "#     matched_image = image_filenames[retrieved_indices[0][0]]\n",
    "    \n",
    "#     print(f\"üñºÔ∏è Original Flowchart: {original_image}\")\n",
    "#     print(f\"üîç Closest Match: {matched_image}\")\n",
    "#     print(f\"Similarity Score: {np.dot(image_embeddings[index], image_embeddings[retrieved_indices[0][0]])}\")\n",
    "    \n",
    "#     # Show both images\n",
    "#     fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "#     ax[0].imshow(Image.open(os.path.join(image_dir, original_image)))\n",
    "#     ax[0].set_title(\"Original Image\")\n",
    "#     ax[0].axis(\"off\")\n",
    "    \n",
    "#     ax[1].imshow(Image.open(os.path.join(image_dir, matched_image)))\n",
    "#     ax[1].set_title(\"Closest Match\")\n",
    "#     ax[1].axis(\"off\")\n",
    "    \n",
    "#     plt.show()\n",
    "\n",
    "# check_image_embedding()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ef5c1cd-0f45-4618-975d-6d379d74c79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if all image embeddings are unique\n",
    "# unique_embeddings = np.unique(image_embeddings, axis=0)\n",
    "\n",
    "# if unique_embeddings.shape[0] == 1:\n",
    "#     print(\"‚ö†Ô∏è WARNING: All image embeddings are identical! FAISS cannot differentiate them.\")\n",
    "# else:\n",
    "#     print(f\"‚úÖ FAISS has {unique_embeddings.shape[0]} unique image embeddings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "019b9bdf-1990-476b-9466-49693196db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print shapes of stored image embeddings\n",
    "# print(f\"Stored Image Embeddings Shape: {image_embeddings.shape}\")\n",
    "\n",
    "# # Generate a query embedding for comparison\n",
    "# query_embedding = get_query_image_embedding(os.path.join(image_dir, image_filenames[0]))  # Use any image as query\n",
    "# print(f\"Query Image Embedding Shape: {query_embedding.shape}\")\n",
    "\n",
    "# # Print first stored embedding vs query embedding\n",
    "# print(f\"\\nFirst Stored Embedding:\\n{image_embeddings[0][:10]}\")  # Print first 10 values\n",
    "# print(f\"\\nQuery Embedding:\\n{query_embedding[:10]}\")  # Print first 10 values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebf60dc3-dc92-4790-affa-50e372f30d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def debug_faiss_retrieval(index=3):\n",
    "#     \"\"\"Check if FAISS is properly differentiating images.\"\"\"\n",
    "#     query_embedding = image_embeddings[index].reshape(1, -1)  # Use stored image for retrieval test\n",
    "#     _, retrieved_indices = image_index.search(query_embedding, 5)  # Top 5 results\n",
    "\n",
    "#     print(f\"üñºÔ∏è Original Flowchart: {image_filenames[index]}\")\n",
    "#     print(f\"\\nüîç Closest Matches:\")\n",
    "#     for rank, idx in enumerate(retrieved_indices[0]):\n",
    "#         matched_image = image_filenames[idx]\n",
    "#         similarity_score = np.dot(image_embeddings[index], image_embeddings[idx])  # Cosine similarity\n",
    "#         print(f\"{rank + 1}. {matched_image} (Score: {similarity_score:.6f})\")\n",
    "\n",
    "# debug_faiss_retrieval()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
