{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc549157-03fd-47f4-b7ee-e1d78e3d5ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stored document text embeddings in FAISS database.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "import fitz  \n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "### DOCUMENT PROCESSING ###\n",
    "\n",
    "# Load text embedding model\n",
    "text_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a given PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text(\"text\") + \"\\n\"\n",
    "    return text.strip()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Enhance retrieval by applying TF-IDF weighting.\"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
    "    X = vectorizer.fit_transform([text]).toarray()\n",
    "    return \" \".join(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Directory setup\n",
    "base_dir = \"business_flowcharts\"\n",
    "pdf_dir = os.path.join(base_dir, \"documents\")\n",
    "\n",
    "# Read and process PDFs\n",
    "pdf_texts = {}\n",
    "pdf_filenames = sorted([f for f in os.listdir(pdf_dir) if f.endswith(\".pdf\")])\n",
    "\n",
    "pdf_embeddings = []\n",
    "for file in pdf_filenames:\n",
    "    pdf_path = os.path.join(pdf_dir, file)\n",
    "    raw_text = extract_text_from_pdf(pdf_path)\n",
    "    processed_text = preprocess_text(raw_text)\n",
    "    pdf_texts[file] = raw_text\n",
    "    embedding = text_model.encode(processed_text)\n",
    "    pdf_embeddings.append(embedding)\n",
    "\n",
    "# Convert to FAISS-compatible format\n",
    "pdf_embeddings = np.array(pdf_embeddings, dtype=\"float32\")\n",
    "pdf_embeddings /= np.linalg.norm(pdf_embeddings, axis=1, keepdims=True)  # Normalize (IMPORTANT)\n",
    "\n",
    "# Create FAISS index\n",
    "text_index = faiss.IndexFlatL2(pdf_embeddings.shape[1])\n",
    "text_index.add(pdf_embeddings)\n",
    "\n",
    "print(\"‚úÖ Stored document text embeddings in FAISS database.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "234472c5-6981-489f-afda-da8f1d784f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stored flowchart image embeddings in FAISS database.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "### FLOWCHART IMG PROCESSING ###\n",
    "\n",
    "# Load CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"Preprocess an image dynamically while maintaining aspect ratio.\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Resize while keeping aspect ratio\n",
    "    aspect_ratio = image.width / image.height\n",
    "    if aspect_ratio > 1:\n",
    "        new_width = 224\n",
    "        new_height = int(224 / aspect_ratio)\n",
    "    else:\n",
    "        new_height = 224\n",
    "        new_width = int(224 * aspect_ratio)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((new_height, new_width)),  # Maintain aspect ratio\n",
    "        transforms.Pad((0, 0, 224 - new_width, 224 - new_height), fill=(255, 255, 255)),  # Pad with white\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.481, 0.457, 0.408], std=[0.268, 0.261, 0.275]),  # CLIP normalization (ALSO IMPORATNT)\n",
    "    ])\n",
    "\n",
    "    return transform(image).unsqueeze(0)\n",
    "\n",
    "def get_image_embedding(image_path):\n",
    "    \"\"\"Generate an embedding for a flowchart image using CLIP.\"\"\"\n",
    "    image_tensor = preprocess_image(image_path).to(device)\n",
    "    with torch.no_grad():\n",
    "        embedding = clip_model.encode_image(image_tensor).cpu().numpy()\n",
    "    return embedding.flatten()\n",
    "\n",
    "# Directory setup\n",
    "image_dir = os.path.join(base_dir, \"flowcharts\")\n",
    "image_filenames = sorted([f for f in os.listdir(image_dir) if f.endswith(\".png\")])\n",
    "\n",
    "# Process images\n",
    "image_embeddings = [get_image_embedding(os.path.join(image_dir, file)) for file in image_filenames]\n",
    "image_embeddings = np.array(image_embeddings, dtype=\"float32\")\n",
    "image_embeddings /= np.linalg.norm(image_embeddings, axis=1, keepdims=True)  # Normalize\n",
    "\n",
    "# Create FAISS index\n",
    "image_index = faiss.IndexFlatL2(image_embeddings.shape[1])\n",
    "image_index.add(image_embeddings)\n",
    "\n",
    "print(\"‚úÖ Stored flowchart image embeddings in FAISS database.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31fcba5a-5490-47e1-9685-7e66bd084120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "def extract_text_from_image(image_path):\n",
    "    \"\"\"Extract text from a given flowchart image using OCR.\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    extracted_text = pytesseract.image_to_string(image)\n",
    "    return extracted_text.strip()\n",
    "\n",
    "def get_text_embedding(text):\n",
    "    \"\"\"Generate an embedding for extracted text using Sentence-BERT.\"\"\"\n",
    "    return text_model.encode(text)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2285b3dc-6fcf-4600-9620-b7540d81ec4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Retrieval function V2 is ready.\n"
     ]
    }
   ],
   "source": [
    "def get_query_image_embedding(image_path):\n",
    "    \"\"\"Generate a normalized embedding for a query image using CLIP.\"\"\"\n",
    "    image_tensor = preprocess_image(image_path).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embedding = clip_model.encode_image(image_tensor).cpu().numpy()\n",
    "    \n",
    "    return embedding.flatten() / np.linalg.norm(embedding)  # Normalize\n",
    "\n",
    "def retrieve_relevant_data(query, flowchart_imgs=None, top_k=2):\n",
    "    \"\"\"Retrieve the most relevant documents & images for the query using FAISS.\"\"\"\n",
    "\n",
    "    if flowchart_imgs and isinstance(flowchart_imgs, str):\n",
    "        flowchart_imgs = [flowchart_imgs]  # Ensure list format\n",
    "\n",
    "    # Convert query to text embedding\n",
    "    query_embedding = text_model.encode(query).reshape(1, -1)\n",
    "    query_embedding /= np.linalg.norm(query_embedding)  # Normalize\n",
    "\n",
    "    # Search FAISS text database (Retrieve relevant PDFs)\n",
    "    text_distances, text_results = text_index.search(query_embedding, top_k)\n",
    "    retrieved_pdfs = [(pdf_filenames[idx], text_distances[0][i]) for i, idx in enumerate(text_results[0])]\n",
    "\n",
    "    retrieved_images = []\n",
    "    image_scores = []\n",
    "    unknown_images = []  # Store unrecognized images\n",
    "\n",
    "    # Get a set of indexed flowcharts\n",
    "    indexed_flowcharts = set(image_filenames)  # All pre-indexed flowcharts\n",
    "\n",
    "    if flowchart_imgs:\n",
    "        for flowchart_img in flowchart_imgs:\n",
    "            # If the image is outside of the indexed directory, mark as NEW\n",
    "            if os.path.basename(flowchart_img) not in indexed_flowcharts:\n",
    "                print(f\"üÜï Marking '{flowchart_img}' as [NEW FLOWCHART] (not in FAISS index)\")\n",
    "                unknown_images.append(flowchart_img)\n",
    "                continue  # Skip FAISS retrieval\n",
    "\n",
    "            # Normal FAISS retrieval for known images\n",
    "            extracted_text = extract_text_from_image(flowchart_img)\n",
    "            text_embedding = get_text_embedding(extracted_text)\n",
    "\n",
    "            # Retrieve documents based on extracted image text\n",
    "            text_distances, text_results = text_index.search(text_embedding.reshape(1, -1), top_k)\n",
    "            retrieved_pdfs += [(pdf_filenames[idx], text_distances[0][i]) for i, idx in enumerate(text_results[0])]\n",
    "\n",
    "            # Get CLIP image embedding\n",
    "            image_query_embedding = get_query_image_embedding(flowchart_img)\n",
    "\n",
    "            # Retrieve similar images from FAISS (with distance scores)\n",
    "            image_distances, image_results = image_index.search(image_query_embedding.reshape(1, -1), top_k)\n",
    "\n",
    "            if image_results[0][0] >= 0:  # If valid results exist\n",
    "                retrieved_images += [image_filenames[idx] for idx in image_results[0]]\n",
    "                image_scores += list(image_distances[0])  # Store distances\n",
    "            else:\n",
    "                unknown_images.append(flowchart_img)  # Mark as unknown\n",
    "\n",
    "    # Sort PDFs & images by FAISS similarity scores\n",
    "    retrieved_pdfs = sorted(set(retrieved_pdfs), key=lambda x: x[1])[:top_k]\n",
    "    retrieved_pdfs = [pdf for pdf, _ in retrieved_pdfs]  # Keep only filenames\n",
    "\n",
    "    # Sort images by similarity scores\n",
    "    image_sorted = sorted(zip(retrieved_images, image_scores), key=lambda x: x[1])[:top_k]\n",
    "    retrieved_images = [img for img, _ in image_sorted]  # Extract sorted filenames\n",
    "\n",
    "    # Append unknown images explicitly labeled as \"New Flowchart (V2)\"\n",
    "    retrieved_images += [f\"[NEW FLOWCHART] {img}\" for img in unknown_images]\n",
    "\n",
    "    return retrieved_pdfs, retrieved_images\n",
    "\n",
    "\n",
    "print(\"‚úÖ Retrieval function V2 is ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd22c25b-b1a2-459b-926b-60adfc08d7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32a82d622b484e04897549d6090d0980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n",
    "\n",
    "# Load Qwen model\n",
    "qwen_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\", \n",
    "    torch_dtype=torch.bfloat16,  \n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "qwen_processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "031ac69c-2195-4f38-86df-414c6e47d450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def debug_query_qwen_with_rag(query, flowchart_img=None, top_k=2):\n",
    "#     \"\"\"DEBUG MODE - Retrieve relevant flowchart data (text & images) and query Qwen for an AI-generated response.\"\"\"\n",
    "    \n",
    "#     # Retrieve relevant PDFs (text) & Flowcharts (images)\n",
    "#     retrieved_pdfs, retrieved_images = retrieve_relevant_data(query, flowchart_img, top_k)\n",
    "    \n",
    "#     # Extract text from retrieved PDFs\n",
    "#     context = \"\\n\".join([pdf_texts[pdf] for pdf in retrieved_pdfs])\n",
    "\n",
    "#     # Use retrieved flowchart image if `flowchart_img` is None\n",
    "#     image_path = flowchart_img if flowchart_img else os.path.join(image_dir, retrieved_images[0])\n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "#     # Define the user message (Injecting retrieved context)\n",
    "#     messages = [\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": [\n",
    "#                 {\"type\": \"image\", \"image\": image},\n",
    "#                 {\"type\": \"text\", \"text\": f\"Attached is document providing context. Based on the provided image and textual information, please analyze the content and generate a response that accurately addresses the user's inquiry.\\n\\nContext:\\n{context}\\n\\nQuery: {query}\"},\n",
    "#             ],\n",
    "#         }\n",
    "#     ]\n",
    "    \n",
    "#     # Format input for Qwen\n",
    "#     text = qwen_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "#     inputs = qwen_processor(\n",
    "#         text=[text],\n",
    "#         images=[image],\n",
    "#         padding=True,\n",
    "#         return_tensors=\"pt\",\n",
    "#     ).to(qwen_model.device)\n",
    "    \n",
    "#     # Generate response\n",
    "#     with torch.no_grad():\n",
    "#         output_ids = qwen_model.generate(inputs, max_new_tokens=512)\n",
    "    \n",
    "#     # Decode response\n",
    "#     response_text = \"========\\n\\n\".join(qwen_processor.batch_decode(output_ids, skip_special_tokens=False))\n",
    "    \n",
    "#     return response_text, retrieved_pdfs, retrieved_images\n",
    "\n",
    "# print(\"‚úÖ DEBUG Qwen RAG system is ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cee3aa1f-4093-4a85-bf0b-08e159d6f64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Qwen RAG V2 system is ready.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def query_qwen_with_rag(query, flowchart_imgs=None, top_k=2):\n",
    "    \"\"\"Retrieve relevant flowchart data (text & images) and query Qwen for an AI-generated response.\"\"\"\n",
    "\n",
    "    if flowchart_imgs and isinstance(flowchart_imgs, str):\n",
    "        flowchart_imgs = [flowchart_imgs]  # Ensure list format\n",
    "\n",
    "    # Retrieve relevant PDFs (text) & Flowcharts (images)\n",
    "    retrieved_pdfs, retrieved_images = retrieve_relevant_data(query, flowchart_imgs, top_k)\n",
    "    \n",
    "    # Extract text from retrieved PDFs\n",
    "    context = \"\\n\".join([pdf_texts[pdf] for pdf in retrieved_pdfs])\n",
    "\n",
    "    # Extract text from each flowchart individually\n",
    "    flowchart_texts = []\n",
    "    descriptions = []\n",
    "    valid_images = []  # Store valid images\n",
    "\n",
    "    if flowchart_imgs:\n",
    "        for img_path in flowchart_imgs:\n",
    "            if img_path.startswith(\"[NEW FLOWCHART]\"):\n",
    "                descriptions.append(f\"‚ö†Ô∏è This is a newly provided flowchart: {img_path.replace('[NEW FLOWCHART] ', '')}\")\n",
    "                flowchart_texts.append(f\"Flowchart {len(flowchart_texts) + 1} (User-Provided Flowchart):\\n\\n[Unable to extract full text, refer to image]\")\n",
    "                continue\n",
    "\n",
    "            if not os.path.exists(img_path):\n",
    "                print(f\"‚ö†Ô∏è Warning: Image '{img_path}' not found. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Extract OCR text\n",
    "            extracted_text = extract_text_from_image(img_path)\n",
    "            flowchart_texts.append(f\"Flowchart {len(flowchart_texts) + 1}:\\n\\n{extracted_text}\")\n",
    "\n",
    "            # Load valid image\n",
    "            valid_images.append(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "    # Ensure the AI processes both flowcharts separately\n",
    "    flowchart_section = \"\\n\\n\".join(flowchart_texts)\n",
    "\n",
    "    # Build user message content\n",
    "    content = [\n",
    "        *([{\"type\": \"image\", \"image\": img} for img in valid_images]),  # Attach images\n",
    "        *([{\"type\": \"text\", \"text\": desc} for desc in descriptions]),  # Describe unknown images\n",
    "        {\"type\": \"text\", \"text\": f\"Context:\\n{context}\\n\\n{flowchart_section}\\n\\n{query}\"}\n",
    "    ]\n",
    "\n",
    "    # Ensure `inputs` is correctly formatted\n",
    "    if not valid_images:  # Handle text-only query\n",
    "        print(\"üîπ No images detected, processing as a pure text query.\")\n",
    "        text_input = qwen_processor.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": query}]}], \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        inputs = qwen_processor(\n",
    "            text=[text_input],  # Ensure list format\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(qwen_model.device)\n",
    "    else:\n",
    "        text_input = qwen_processor.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": content}], \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        inputs = qwen_processor(\n",
    "            text=[text_input],  # Ensure list format\n",
    "            images=valid_images,  # Provide images only if available\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(qwen_model.device)\n",
    "\n",
    "    # Check if inputs is correctly formatted before generating\n",
    "    if not hasattr(inputs, \"input_ids\"):\n",
    "        print(\"‚ö†Ô∏è Error: Inputs are incorrectly formatted. Skipping generation.\")\n",
    "        return \"Error: Invalid input formatting\", retrieved_pdfs, retrieved_images\n",
    "\n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        output_ids = qwen_model.generate(**inputs, max_new_tokens=1024)\n",
    "\n",
    "    # Decode response\n",
    "    response_text = qwen_processor.batch_decode(output_ids, skip_special_tokens=False)[0]\n",
    "\n",
    "    # Extract Assistant's Response Only\n",
    "    match = re.search(r\"assistant\\s*\\n(.*)\", response_text, re.DOTALL)\n",
    "    cleaned_response = match.group(1).strip() if match else response_text.strip()\n",
    "\n",
    "    return cleaned_response, retrieved_pdfs, retrieved_images\n",
    "\n",
    "\n",
    "print(\"‚úÖ Qwen RAG V2 system is ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ae4f055-8f6e-48fb-b117-c4ae3962c608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Qwen's Response (With RAG):\n",
      "In the Incident Management flowchart provided, the classification of incidents is performed by the support team. Specifically, the \"Classify Incident\" step involves the support team categorizing the issue based on its nature, urgency, and complexity to determine the appropriate response.<|im_end|>\n",
      "üîç Retrieved Documents: ['6_Incident_Management.pdf', '10_Help_Desk_Ticketing.pdf']\n",
      "üñºÔ∏è Retrieved Flowcharts: ['6_incident_management.png', '1_customer_support.png']\n",
      "‚è≥ Execution Time: 2.00 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Define the test query and flowchart image\n",
    "# user_query = \"Can you see what flowchart this is? Can you list out the steps and give an explanation?\"\n",
    "# flowchart_img = \"business_flowcharts/flowcharts/14_quality_control.png\"\n",
    "# flowchart_img = \"business_flowcharts/flowcharts/17_medical_diagnosis.png\"\n",
    "user_query = \"Who classifies the incidents here?\"\n",
    "flowchart_img = \"business_flowcharts/flowcharts/6_incident_management.png\"\n",
    "# In this cybersecurity flowchart, explain the difference in action should the threat be labelled as critical or not.\n",
    "# Measure execution time\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Query Qwen with RAG\n",
    "qwen_response_rag, retrieved_pdfs, retrieved_images = query_qwen_with_rag(user_query, flowchart_img)\n",
    "\n",
    "# Calculate total time taken\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nü§ñ Qwen's Response (With RAG):\\n{qwen_response_rag}\")\n",
    "print(f\"üîç Retrieved Documents: {retrieved_pdfs}\")\n",
    "print(f\"üñºÔ∏è Retrieved Flowcharts: {retrieved_images}\")\n",
    "print(f\"‚è≥ Execution Time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43813de-cd21-4e02-b106-aa6211d10d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bee64c3f-82d7-479a-aa95-a604f4d06471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Comparison RAG IS FIXED I HOPE\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def compare_query_qwen_with_rag(query, flowchart_imgs=None, top_k=2):\n",
    "    \"\"\"Retrieve relevant flowchart data (text & images) and query Qwen for an AI-generated response.\"\"\"\n",
    "\n",
    "    if flowchart_imgs and isinstance(flowchart_imgs, str):\n",
    "        flowchart_imgs = [flowchart_imgs]  # Ensure list format\n",
    "\n",
    "    # Retrieve relevant PDFs (text) & Flowcharts (images)\n",
    "    retrieved_pdfs, retrieved_images = retrieve_relevant_data(query, flowchart_imgs, top_k)\n",
    "    \n",
    "    # Extract text from retrieved PDFs\n",
    "    context = \"\\n\".join([pdf_texts[pdf] for pdf in retrieved_pdfs])\n",
    "\n",
    "    # Extract text from each flowchart individually\n",
    "    flowchart_texts = []\n",
    "    descriptions = []\n",
    "    valid_images = []  # Store valid images\n",
    "\n",
    "    if flowchart_imgs:\n",
    "        for img_path in flowchart_imgs:\n",
    "            if img_path.startswith(\"[NEW FLOWCHART]\"):\n",
    "                descriptions.append(f\"‚ö†Ô∏è This is a newly provided flowchart: {img_path.replace('[NEW FLOWCHART] ', '')}\")\n",
    "                flowchart_texts.append(f\"Flowchart {len(flowchart_texts) + 1} (User-Provided Flowchart):\\n\\n[Unable to extract full text, refer to image]\")\n",
    "                continue\n",
    "\n",
    "            if not os.path.exists(img_path):\n",
    "                print(f\"‚ö†Ô∏è Warning: Image '{img_path}' not found. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Extract OCR text\n",
    "            extracted_text = extract_text_from_image(img_path)\n",
    "            flowchart_texts.append(f\"Flowchart {len(flowchart_texts) + 1}:\\n\\n{extracted_text}\")\n",
    "\n",
    "            # Load valid image\n",
    "            valid_images.append(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "    # Ensure the AI processes both flowcharts separately\n",
    "    flowchart_section = \"\\n\\n\".join(flowchart_texts)\n",
    "\n",
    "    # Define the user message (Injecting retrieved context)\n",
    "    content = [\n",
    "        *([{\"type\": \"image\", \"image\": img} for img in valid_images]),  # Attach images\n",
    "        *([{\"type\": \"text\", \"text\": desc} for desc in descriptions]),  # Describe unknown images\n",
    "        {\"type\": \"text\", \"text\": f\"Below are the details extracted from the flowcharts provided:\\n\\n{flowchart_section}\\n\\nCompare the flowcharts carefully. Provide step-by-step differences and explain any structural changes.\\n\\n{query}\"},\n",
    "    ]\n",
    "\n",
    "    # Ensure `inputs` is correctly formatted\n",
    "    if not valid_images:  # Handle text-only query\n",
    "        print(\"üîπ No images detected, processing as a pure text query.\")\n",
    "        text_input = qwen_processor.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": query}]}], \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        inputs = qwen_processor(\n",
    "            text=[text_input],  # Ensure list format\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(qwen_model.device)\n",
    "    else:\n",
    "        text_input = qwen_processor.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": content}], \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        inputs = qwen_processor(\n",
    "            text=[text_input],  # Ensure list format\n",
    "            images=valid_images,  # Provide images only if available\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(qwen_model.device)\n",
    "\n",
    "    # Check if inputs is correctly formatted before generating\n",
    "    if not hasattr(inputs, \"input_ids\"):\n",
    "        print(\"‚ö†Ô∏è Error: Inputs are incorrectly formatted. Skipping generation.\")\n",
    "        return \"Error: Invalid input formatting\", retrieved_pdfs, retrieved_images\n",
    "\n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        output_ids = qwen_model.generate(**inputs, max_new_tokens=1024)\n",
    "\n",
    "    # Decode response\n",
    "    response_text = qwen_processor.batch_decode(output_ids, skip_special_tokens=False)[0]\n",
    "\n",
    "    # Extract Assistant's Response Only\n",
    "    match = re.search(r\"assistant\\s*\\n(.*)\", response_text, re.DOTALL)\n",
    "    cleaned_response = match.group(1).strip() if match else response_text.strip()\n",
    "\n",
    "    return cleaned_response, retrieved_pdfs, retrieved_images\n",
    "\n",
    "print(\"‚úÖ Comparison RAG IS FIXED I HOPE\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "842614af-fc72-404b-8bbe-072c7bc1db2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üÜï Marking '6_incident_management_v2.png' as [NEW FLOWCHART] (not in FAISS index)\n",
      "\n",
      "ü§ñ Qwen's Response (With RAG):\n",
      "Certainly! Let's compare the two flowcharts step by step:\n",
      "\n",
      "### Flowchart 1:\n",
      "1. **Start**\n",
      "2. **User Reports Issue**\n",
      "3. **Classify Incident**\n",
      "4. **High Impact?**\n",
      "   - If Yes: Escalate to Higher Support\n",
      "   - If No: Assign to Support Team\n",
      "5. **Resolve Issue**\n",
      "6. **Issue Resolved?**\n",
      "   - If Yes: Close Ticket\n",
      "   - If No: Escalate to Higher Support\n",
      "\n",
      "### Flowchart 2:\n",
      "1. **Start**\n",
      "2. **User Reports Issue**\n",
      "3. **Initial Triage**\n",
      "4. **Classify Incident**\n",
      "5. **Automated Resolution?**\n",
      "   - If Yes: Resolve Issue\n",
      "   - If No:\n",
      "     - High Impact?\n",
      "       - If Yes: Escalate to Higher Support\n",
      "       - If No: Assign to Support Team\n",
      "     - If Yes: Critical?\n",
      "       - If Yes: Escalate to Higher Support\n",
      "       - If No: Assign to Support Team\n",
      "6. **Resolve Issue**\n",
      "7. **Issue Resolved?**\n",
      "   - If Yes: Close Ticket\n",
      "   - If No: Follow-up Needed?\n",
      "\n",
      "### Differences:\n",
      "\n",
      "1. **Initial Triage**:\n",
      "   - **Flowchart 1**: No initial triage step.\n",
      "   - **Flowchart 2**: Added an \"Initial Triage\" step before classifying incidents.\n",
      "\n",
      "2. **Automated Resolution**:\n",
      "   - **Flowchart 1**: Does not mention automated resolution.\n",
      "   - **Flowchart 2**: Added an \"Automated Resolution?\" step, which is a decision point that determines whether to resolve the issue automatically or manually.\n",
      "\n",
      "3. **High Impact Decision**:\n",
      "   - **Flowchart 1**: Directly routes high-impact issues to escalation.\n",
      "   - **Flowchart 2**: Routes high-impact issues to escalation only if they are critical.\n",
      "\n",
      "4. **Critical Decision**:\n",
      "   - **Flowchart 1**: No critical decision point.\n",
      "   - **Flowchart 2**: Added a \"Critical?\" decision point for high-impact issues.\n",
      "\n",
      "5. **Escalation**:\n",
      "   - **Flowchart 1**: Escalates high-impact issues directly.\n",
      "   - **Flowchart 2**: Escalates high-impact issues only if they are critical.\n",
      "\n",
      "6. **Follow-up Needed**:\n",
      "   - **Flowchart 1**: No follow-up needed check.\n",
      "   - **Flowchart 2**: Added a \"Follow-up Needed?\" check after resolving the issue.\n",
      "\n",
      "### Summary of Changes:\n",
      "- **Flowchart 2** includes additional steps such as initial triage, automated resolution, critical decision points, and a follow-up check.\n",
      "- **Flowchart 2** also has a more detailed structure with multiple decision points (e.g., Automated Resolution?, High Impact?, Critical?) compared to the simpler structure of **Flowchart 1**.\n",
      "- The overall process in **Flowchart 2** appears to be more comprehensive and detailed, potentially covering more scenarios and ensuring better management of incidents based on their severity and impact.<|im_end|>\n",
      "üîç Retrieved Documents: ['6_Incident_Management.pdf', '10_Help_Desk_Ticketing.pdf']\n",
      "üñºÔ∏è Retrieved Flowcharts: ['6_incident_management.png', '1_customer_support.png', '[NEW FLOWCHART] 6_incident_management_v2.png']\n",
      "‚è≥ Execution Time: 11.48 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Define the test query and multiple flowchart images\n",
    "# user_query = \"Can you compare these flowcharts in detail? What are the difference between the first and the second one. I'm working on the second one to give more detail.\"\n",
    "user_query = \"Can you compare these flowcharts in detail? What are the difference between the first and the second one. I'm working on the second one to give more detail. Summarize each comparison.\"\n",
    "flowchart_imgs = [\n",
    "    \"business_flowcharts/flowcharts/6_incident_management.png\",\n",
    "    \"6_incident_management_v2.png\"\n",
    "]\n",
    "\n",
    "# flowchart_imgs = [\n",
    "#     \"business_flowcharts/flowcharts/9_cybersecurity_incident_response.png\",\n",
    "#     \"9_cybersecurity_incident_response_v2.png\"\n",
    "# ]\n",
    "# Measure execution time\n",
    "start_time = time.time()\n",
    "\n",
    "# Query Qwen with RAG\n",
    "qwen_response_rag, retrieved_pdfs, retrieved_images = compare_query_qwen_with_rag(user_query, flowchart_imgs)\n",
    "\n",
    "# Calculate total time taken\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nü§ñ Qwen's Response (With RAG):\\n{qwen_response_rag}\")\n",
    "print(f\"üîç Retrieved Documents: {retrieved_pdfs}\")\n",
    "print(f\"üñºÔ∏è Retrieved Flowcharts: {retrieved_images}\")\n",
    "print(f\"‚è≥ Execution Time: {execution_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5094d2e1-0daf-40fd-a084-8ab03b3b683f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26fda206-ca13-4497-9313-462900efb1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test saved documents in FAISS database\n",
    "# def view_stored_pdfs():\n",
    "#     \"\"\"Display all stored PDFs and their extracted text.\"\"\"\n",
    "#     for filename, text in pdf_texts.items():\n",
    "#         print(f\"üìÑ PDF: {filename}\\n\")\n",
    "#         print(f\"Extracted Content:\\n{text[:1000]}\")  # Show first 1000 characters\n",
    "#         print(\"=\"*80)\n",
    "\n",
    "# view_stored_pdfs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "766448a9-e470-49d0-9eff-3adef9c37473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Test saved images in FAISS database\n",
    "\n",
    "# def view_stored_flowcharts():\n",
    "#     \"\"\"Display all stored flowchart images and their filenames.\"\"\"\n",
    "#     for filename in image_filenames:\n",
    "#         image_path = os.path.join(image_dir, filename)\n",
    "#         image = Image.open(image_path)\n",
    "        \n",
    "#         plt.figure(figsize=(5, 5))\n",
    "#         plt.imshow(image)\n",
    "#         plt.axis(\"off\")\n",
    "#         plt.title(f\"üñºÔ∏è Flowchart: {filename}\")\n",
    "#         plt.show()\n",
    "\n",
    "# view_stored_flowcharts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9e5fcdf-793f-41ba-a300-ff851d41b6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_pdf_embedding(index=11):\n",
    "#     \"\"\"Check stored text embedding by retrieving the closest match for a given PDF.\"\"\"\n",
    "#     query_embedding = pdf_embeddings[index].reshape(1, -1)\n",
    "#     _, retrieved_indices = text_index.search(query_embedding, 1)\n",
    "    \n",
    "#     original_pdf = pdf_filenames[index]\n",
    "#     matched_pdf = pdf_filenames[retrieved_indices[0][0]]\n",
    "    \n",
    "#     print(f\"üìÑ Original PDF: {original_pdf}\")\n",
    "#     print(f\"üîç Closest Match: {matched_pdf}\")\n",
    "#     print(f\"Similarity Score: {np.dot(pdf_embeddings[index], pdf_embeddings[retrieved_indices[0][0]])}\")\n",
    "    \n",
    "# check_pdf_embedding()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b23561cd-0ea7-427f-8c03-41c278131caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def check_image_embedding(index=3):\n",
    "#     \"\"\"Check stored image embedding by retrieving the closest match for a given flowchart.\"\"\"\n",
    "#     query_embedding = image_embeddings[index].reshape(1, -1)\n",
    "#     _, retrieved_indices = image_index.search(query_embedding, 1)\n",
    "    \n",
    "#     original_image = image_filenames[index]\n",
    "#     matched_image = image_filenames[retrieved_indices[0][0]]\n",
    "    \n",
    "#     print(f\"üñºÔ∏è Original Flowchart: {original_image}\")\n",
    "#     print(f\"üîç Closest Match: {matched_image}\")\n",
    "#     print(f\"Similarity Score: {np.dot(image_embeddings[index], image_embeddings[retrieved_indices[0][0]])}\")\n",
    "    \n",
    "#     # Show both images\n",
    "#     fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "#     ax[0].imshow(Image.open(os.path.join(image_dir, original_image)))\n",
    "#     ax[0].set_title(\"Original Image\")\n",
    "#     ax[0].axis(\"off\")\n",
    "    \n",
    "#     ax[1].imshow(Image.open(os.path.join(image_dir, matched_image)))\n",
    "#     ax[1].set_title(\"Closest Match\")\n",
    "#     ax[1].axis(\"off\")\n",
    "    \n",
    "#     plt.show()\n",
    "\n",
    "# check_image_embedding()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ef5c1cd-0f45-4618-975d-6d379d74c79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if all image embeddings are unique\n",
    "# unique_embeddings = np.unique(image_embeddings, axis=0)\n",
    "\n",
    "# if unique_embeddings.shape[0] == 1:\n",
    "#     print(\"‚ö†Ô∏è WARNING: All image embeddings are identical! FAISS cannot differentiate them.\")\n",
    "# else:\n",
    "#     print(f\"‚úÖ FAISS has {unique_embeddings.shape[0]} unique image embeddings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "019b9bdf-1990-476b-9466-49693196db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print shapes of stored image embeddings\n",
    "# print(f\"Stored Image Embeddings Shape: {image_embeddings.shape}\")\n",
    "\n",
    "# # Generate a query embedding for comparison\n",
    "# query_embedding = get_query_image_embedding(os.path.join(image_dir, image_filenames[0]))  # Use any image as query\n",
    "# print(f\"Query Image Embedding Shape: {query_embedding.shape}\")\n",
    "\n",
    "# # Print first stored embedding vs query embedding\n",
    "# print(f\"\\nFirst Stored Embedding:\\n{image_embeddings[0][:10]}\")  # Print first 10 values\n",
    "# print(f\"\\nQuery Embedding:\\n{query_embedding[:10]}\")  # Print first 10 values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebf60dc3-dc92-4790-affa-50e372f30d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def debug_faiss_retrieval(index=3):\n",
    "#     \"\"\"Check if FAISS is properly differentiating images.\"\"\"\n",
    "#     query_embedding = image_embeddings[index].reshape(1, -1)  # Use stored image for retrieval test\n",
    "#     _, retrieved_indices = image_index.search(query_embedding, 5)  # Top 5 results\n",
    "\n",
    "#     print(f\"üñºÔ∏è Original Flowchart: {image_filenames[index]}\")\n",
    "#     print(f\"\\nüîç Closest Matches:\")\n",
    "#     for rank, idx in enumerate(retrieved_indices[0]):\n",
    "#         matched_image = image_filenames[idx]\n",
    "#         similarity_score = np.dot(image_embeddings[index], image_embeddings[idx])  # Cosine similarity\n",
    "#         print(f\"{rank + 1}. {matched_image} (Score: {similarity_score:.6f})\")\n",
    "\n",
    "# debug_faiss_retrieval()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
