{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faf7280e-1b18-49c5-b236-3ef56ccea351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: flash_attn\n",
      "Version: 2.7.4.post1\n",
      "Summary: Flash Attention: Fast and Memory-Efficient Exact Attention\n",
      "Home-page: https://github.com/Dao-AILab/flash-attention\n",
      "Author: Tri Dao\n",
      "Author-email: tri@tridao.me\n",
      "License: \n",
      "Location: /home/capstone_student/Documents/sinarmas_project_flowchart/venv/lib/python3.12/site-packages\n",
      "Requires: einops, torch\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "#!pip install qwen_vl_utils\n",
    "#!pip install sentence-transformers faiss-cpu transformers\n",
    "#!pip install torchvision\n",
    "#!pip install accelerate\n",
    "!pip show flash_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c2c8ab2-a288-4000-8155-863de3d57d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory: 24576 MB\n",
      "Used GPU Memory: 943 MB\n",
      "Free GPU Memory: 23203 MB\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def check_cuda_memory():\n",
    "    try:\n",
    "        output = subprocess.check_output([\"nvidia-smi\", \"--query-gpu=memory.total,memory.used,memory.free\", \"--format=csv,nounits,noheader\"])\n",
    "        total, used, free = map(int, output.decode(\"utf-8\").strip().split(\"\\n\")[0].split(\", \"))\n",
    "        print(f\"Total GPU Memory: {total} MB\")\n",
    "        print(f\"Used GPU Memory: {used} MB\")\n",
    "        print(f\"Free GPU Memory: {free} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "check_cuda_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b9957fc-4a08-4382-ba00-0cca89fb9bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    '''\n",
    "    A[Purchase Order Received];\n",
    "    B{Current Customer?};\n",
    "    C[hello world];\n",
    "    D{Customer from US};\n",
    "    E[Process New Customer Record];\n",
    "    F[Submit to Controller for Approval];\n",
    "    G[Input Order];\n",
    "    H[Delete Order];\n",
    "    A --> B;\n",
    "    B -- Yes --> C;\n",
    "    B -- No --> D;\n",
    "    D -- Yes --> E;\n",
    "    D -- No --> F;\n",
    "    E --> G;\n",
    "    F --> G;\n",
    "    C --> H;\n",
    "    G --> H;\n",
    "    '''\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16b718b6-0cbf-4778-a3d4-58c30b16cfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 embeddings.\n"
     ]
    }
   ],
   "source": [
    "# Convert documents to embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "document_embeddings = [embed_model.encode(doc) for doc in documents]\n",
    "\n",
    "print(f\"Generated {len(document_embeddings)} embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "089f4f88-53a2-48f4-a85b-c01b5d3dd8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored document embeddings in FAISS vector database.\n"
     ]
    }
   ],
   "source": [
    "# Convert embeddings to NumPy array and add to FAISS index\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "dimension = len(document_embeddings[0])  # Vector size \n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "vectors = np.array(document_embeddings, dtype=\"float32\")\n",
    "index.add(vectors)\n",
    "\n",
    "print(\"Stored document embeddings in FAISS vector database.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f884a454-3fc8-4336-93d8-005aa1046365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b665578447644189a76cc328f382e9b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code from https://github.com/QwenLM/Qwen2.5-VL, adjusted with 3B version\n",
    "\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# default: Load the model on the available device(s)\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\", \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a19774ba-bdbf-4d6a-b895-2bcbf3cb74cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DEBUG] Retrieved Context:\n",
      "1. \n",
      "    A[Purchase Order Received];\n",
      "    B{Current Customer?};\n",
      "    C[hello world];\n",
      "    D{Customer from US};\n",
      "    E[Process New Customer Record];\n",
      "    F[Submit to Controller for Approval];\n",
      "    G[Input Order];\n",
      "    H[Delete Order];\n",
      "    A --> B;\n",
      "    B -- Yes --> C;\n",
      "    B -- No --> D;\n",
      "    D -- Yes --> E;\n",
      "    D -- No --> F;\n",
      "    E --> G;\n",
      "    F --> G;\n",
      "    C --> H;\n",
      "    G --> H;\n",
      "    \n",
      "2. \n",
      "    A[Purchase Order Received];\n",
      "    B{Current Customer?};\n",
      "    C[hello world];\n",
      "    D{Customer from US};\n",
      "    E[Process New Customer Record];\n",
      "    F[Submit to Controller for Approval];\n",
      "    G[Input Order];\n",
      "    H[Delete Order];\n",
      "    A --> B;\n",
      "    B -- Yes --> C;\n",
      "    B -- No --> D;\n",
      "    D -- Yes --> E;\n",
      "    D -- No --> F;\n",
      "    E --> G;\n",
      "    F --> G;\n",
      "    C --> H;\n",
      "    G --> H;\n",
      "    \n",
      "\n",
      "=== 💬 AI with RAG Response ===\n",
      "system\n",
      "Use the following information to answer the question. Prioritize this information over other knowledge\n",
      "\n",
      "Result:\n",
      "system\n",
      "You are a helpful assistant.\n",
      "context\n",
      "Flowchart:\n",
      "    A[Purchase Order Received];\n",
      "    B{Current Customer?};\n",
      "    C[hello world];\n",
      "    D{Customer from US};\n",
      "    E[Process New Customer Record];\n",
      "    F[Submit to Controller for Approval];\n",
      "    G[Input Order];\n",
      "    H[Delete Order];\n",
      "    A --> B;\n",
      "    B -- Yes --> C;\n",
      "    B -- No --> D;\n",
      "    D -- Yes --> E;\n",
      "    D -- No --> F;\n",
      "    E --> G;\n",
      "    F --> G;\n",
      "    C --> H;\n",
      "    G --> H;\n",
      "    \n",
      "There are 17 lines in the flowchart\n",
      "system\n",
      "\n",
      "\n",
      "Here are instructions on how to read the flowcharts:\n",
      "The first few lines dictate each node's labels. Each line, ended with a semicolon(;), denotes the label like so:\n",
      "(X)[(label)], where (X) is the symbol that will be used to reference this node, and (label) is the text in that node.\n",
      "Square brackets ([]) denotes a process node, while curly braces ({}) denotes a decision node.\n",
      "The rest of the lines denote the flow of operations.\n",
      "It is the format:\n",
      "(X) --> (Y) \n",
      "which denotes that node Y happens after node X.\n",
      "Or the format:\n",
      "(X) -- (Yes/No) --> (Y),\n",
      "where X is a decision node, and Y happens if the condition (Yes/No) matches.\n",
      "Do not answer the user query. Explain the flowchart first line by line first. Explain the reasoning behind each line.\n",
      "Search for the line with the relevant label to the user query.\n",
      "Then, search the line with the symbol relevant to the user query.\n",
      "Then answer the query.\n",
      "        \n",
      "user\n",
      "What comes after 'Input Order'? Prioritize answering the system prompt first\n",
      "assistant\n",
      "Let's break down the flowchart step by step:\n",
      "\n",
      "1. **A[Purchase Order Received]**: This is the initial step where a purchase order is received.\n",
      "\n",
      "2. **B{Current Customer?}**: This is a decision node asking whether the customer is current or not.\n",
      "\n",
      "3. **C[hello world]**: This is a process node that simply outputs \"hello world\".\n",
      "\n",
      "4. **D{Customer from US}**: This is another decision node asking if the customer is from the United States.\n",
      "\n",
      "5. **E[Process New Customer Record]**: If the customer is new, this is a process node that processes a new customer record.\n",
      "\n",
      "6. **F[Submit to Controller for Approval]**: If the customer is not new, this is a process node that submits the order to the controller for approval.\n",
      "\n",
      "7. **G[Input Order]**: This is a process node that inputs the order.\n",
      "\n",
      "8. **H[Delete Order]**: This is a process node that deletes the order.\n",
      "\n",
      "9. **A --> B**: The flow moves from the Purchase Order Received node to the Current Customer? decision node.\n",
      "\n",
      "10. **B -- Yes --> C**: If the customer is current, the flow moves to the hello world process node.\n",
      "\n",
      "11. **B -- No --> D**: If the customer is not current, the flow moves to the Customer from US decision node.\n",
      "\n",
      "12. **D -- Yes --> E**: If the customer is from the United States, the flow moves to the Process New Customer Record process node.\n",
      "\n",
      "13. **D -- No --> F**: If the customer is not from the United States, the flow moves to the Submit to Controller for Approval process node.\n",
      "\n",
      "14. **E --> G**: If the customer is new, the flow moves to the Input Order process node.\n",
      "\n",
      "15. **F --> G**: If the customer is not new, the flow moves to the Input Order process node.\n",
      "\n",
      "16. **C --> H**: The flow moves from the hello world process node to the Delete Order process node.\n",
      "\n",
      "17. **G --> H**: The flow moves from the Input Order process node to the Delete Order process node.\n",
      "\n",
      "Now, let's address the user's query:\n",
      "\n",
      "**Query:** What comes after 'Input Order'?\n",
      "\n",
      "**Answer:** After the Input Order process node, the flow can go to either the Delete Order process node or the Delete Order process node.\n",
      "====\n",
      "=======================\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Function to retrieve the most relevant document based on the user's actual question\n",
    "def retrieve_relevant_text(user_question, top_k=2):\n",
    "    query_embedding = embed_model.encode(user_question).reshape(1, -1)\n",
    "    _, retrieved_indices = index.search(query_embedding, top_k)\n",
    "    retrieved_docs = [documents[i] for i in retrieved_indices[0]]\n",
    "    return retrieved_docs\n",
    "\n",
    "\n",
    "# Load Qwen model and processor\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n",
    "\n",
    "# SET USER QUERY HERE\n",
    "user_question = '''What comes after 'Input Order'? Prioritize answering the system prompt first'''\n",
    "\n",
    "# Use the user's actual question as the query\n",
    "retrieved_context = retrieve_relevant_text(user_question)\n",
    "\n",
    "# Debug output: Print retrieved context separately\n",
    "print(\"\\n[DEBUG] Retrieved Context:\")\n",
    "for i, fact in enumerate(retrieved_context, 1):\n",
    "    print(f\"{i}. {fact}\")\n",
    "\n",
    "# Combine retrieved context into a single string for input\n",
    "retrieved_text = \" \".join(retrieved_context)\n",
    "\n",
    "# 🛠️ Prioritize dataset knowledge but allow fallback to general knowledge\n",
    "messages = [\n",
    "    {\"role\": \"context\", \"content\": \n",
    "     'Flowchart:\\n' + ('\\n'.join(retrieved_context[0].split('\\n')[1:])) + f\"\\nThere are {len(retrieved_context[0].split('\\n'))-2} lines in the flowchart\"},\n",
    "    {\"role\": \"system\", \"content\": (\n",
    "        '''\n",
    "\n",
    "Here are instructions on how to read the flowcharts:\n",
    "The first few lines dictate each node's labels. Each line, ended with a semicolon(;), denotes the label like so:\n",
    "(X)[(label)], where (X) is the symbol that will be used to reference this node, and (label) is the text in that node.\n",
    "Square brackets ([]) denotes a process node, while curly braces ({}) denotes a decision node.\n",
    "The rest of the lines denote the flow of operations.\n",
    "It is the format:\n",
    "(X) --> (Y) \n",
    "which denotes that node Y happens after node X.\n",
    "Or the format:\n",
    "(X) -- (Yes/No) --> (Y),\n",
    "where X is a decision node, and Y happens if the condition (Yes/No) matches.\n",
    "Do not answer the user query. Explain the flowchart first line by line first. Explain the reasoning behind each line.\n",
    "Search for the line with the relevant label to the user query.\n",
    "Then, search the line with the symbol relevant to the user query.\n",
    "Then answer the query.\n",
    "        '''\n",
    "    )}, \n",
    "    {\"role\": \"user\", \"content\": (user_question)},\n",
    "    \n",
    "]\n",
    "\n",
    "# Convert to model input\n",
    "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "# Generate response\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=512)\n",
    "response = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "# Print final output in a structured format\n",
    "print(\"\\n=== 💬 AI with RAG Response ===\")\n",
    "print(\"system\\nUse the following information to answer the question. Prioritize this information over other knowledge\")\n",
    "print()\n",
    "print(\"Result:\")\n",
    "for r in response:\n",
    "    print(r)\n",
    "    print('====')\n",
    "print(\"=======================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0471f20-54e9-43a0-90f9-4f3909caa911",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
