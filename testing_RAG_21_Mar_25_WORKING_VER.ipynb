{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc549157-03fd-47f4-b7ee-e1d78e3d5ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stored document text embeddings in FAISS database.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "import fitz  \n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "### DOCUMENT PROCESSING ###\n",
    "\n",
    "# Load text embedding model\n",
    "text_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a given PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text(\"text\") + \"\\n\"\n",
    "    return text.strip()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Enhance retrieval by applying TF-IDF weighting.\"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
    "    X = vectorizer.fit_transform([text]).toarray()\n",
    "    return \" \".join(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Directory setup\n",
    "base_dir = \"business_flowcharts\"\n",
    "pdf_dir = os.path.join(base_dir, \"documents\")\n",
    "\n",
    "# Read and process PDFs\n",
    "pdf_texts = {}\n",
    "pdf_filenames = sorted([f for f in os.listdir(pdf_dir) if f.endswith(\".pdf\")])\n",
    "\n",
    "pdf_embeddings = []\n",
    "for file in pdf_filenames:\n",
    "    pdf_path = os.path.join(pdf_dir, file)\n",
    "    raw_text = extract_text_from_pdf(pdf_path)\n",
    "    processed_text = preprocess_text(raw_text)\n",
    "    pdf_texts[file] = raw_text\n",
    "    embedding = text_model.encode(processed_text)\n",
    "    pdf_embeddings.append(embedding)\n",
    "\n",
    "# Convert to FAISS-compatible format\n",
    "pdf_embeddings = np.array(pdf_embeddings, dtype=\"float32\")\n",
    "pdf_embeddings /= np.linalg.norm(pdf_embeddings, axis=1, keepdims=True)  # Normalize (IMPORTANT)\n",
    "\n",
    "# Create FAISS index\n",
    "text_index = faiss.IndexFlatL2(pdf_embeddings.shape[1])\n",
    "text_index.add(pdf_embeddings)\n",
    "\n",
    "print(\"‚úÖ Stored document text embeddings in FAISS database.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "234472c5-6981-489f-afda-da8f1d784f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stored flowchart image embeddings in FAISS database.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "### FLOWCHART IMG PROCESSING ###\n",
    "\n",
    "# Load CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"Preprocess an image dynamically while maintaining aspect ratio.\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Resize while keeping aspect ratio\n",
    "    aspect_ratio = image.width / image.height\n",
    "    if aspect_ratio > 1:\n",
    "        new_width = 224\n",
    "        new_height = int(224 / aspect_ratio)\n",
    "    else:\n",
    "        new_height = 224\n",
    "        new_width = int(224 * aspect_ratio)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((new_height, new_width)),  # Maintain aspect ratio\n",
    "        transforms.Pad((0, 0, 224 - new_width, 224 - new_height), fill=(255, 255, 255)),  # Pad with white\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.481, 0.457, 0.408], std=[0.268, 0.261, 0.275]),  # CLIP normalization (ALSO IMPORATNT)\n",
    "    ])\n",
    "\n",
    "    return transform(image).unsqueeze(0)\n",
    "\n",
    "def get_image_embedding(image_path):\n",
    "    \"\"\"Generate an embedding for a flowchart image using CLIP.\"\"\"\n",
    "    image_tensor = preprocess_image(image_path).to(device)\n",
    "    with torch.no_grad():\n",
    "        embedding = clip_model.encode_image(image_tensor).cpu().numpy()\n",
    "    return embedding.flatten()\n",
    "\n",
    "# Directory setup\n",
    "image_dir = os.path.join(base_dir, \"flowcharts\")\n",
    "image_filenames = sorted([f for f in os.listdir(image_dir) if f.endswith(\".png\")])\n",
    "\n",
    "# Process images\n",
    "image_embeddings = [get_image_embedding(os.path.join(image_dir, file)) for file in image_filenames]\n",
    "image_embeddings = np.array(image_embeddings, dtype=\"float32\")\n",
    "image_embeddings /= np.linalg.norm(image_embeddings, axis=1, keepdims=True)  # Normalize\n",
    "\n",
    "# Create FAISS index\n",
    "image_index = faiss.IndexFlatL2(image_embeddings.shape[1])\n",
    "image_index.add(image_embeddings)\n",
    "\n",
    "print(\"‚úÖ Stored flowchart image embeddings in FAISS database.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31fcba5a-5490-47e1-9685-7e66bd084120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "def extract_text_from_image(image_path):\n",
    "    \"\"\"Extract text from a given flowchart image using OCR.\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    extracted_text = pytesseract.image_to_string(image)\n",
    "    return extracted_text.strip()\n",
    "\n",
    "def get_text_embedding(text):\n",
    "    \"\"\"Generate an embedding for extracted text using Sentence-BERT.\"\"\"\n",
    "    return text_model.encode(text)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2285b3dc-6fcf-4600-9620-b7540d81ec4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Retrieval function V2 (With user PDF) is ready.\n"
     ]
    }
   ],
   "source": [
    "def get_query_image_embedding(image_path):\n",
    "    \"\"\"Generate a normalized embedding for a query image using CLIP.\"\"\"\n",
    "    image_tensor = preprocess_image(image_path).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embedding = clip_model.encode_image(image_tensor).cpu().numpy()\n",
    "    \n",
    "    return embedding.flatten() / np.linalg.norm(embedding)  # Normalize\n",
    "\n",
    "def retrieve_relevant_data(query, flowchart_imgs=None, user_provided_pdfs=None, top_k=2):\n",
    "    \"\"\"Retrieve the most relevant documents & images for the query using FAISS.\"\"\"\n",
    "\n",
    "    if flowchart_imgs and isinstance(flowchart_imgs, str):\n",
    "        flowchart_imgs = [flowchart_imgs]\n",
    "\n",
    "    if user_provided_pdfs and isinstance(user_provided_pdfs, str):\n",
    "        user_provided_pdfs = [user_provided_pdfs]\n",
    "\n",
    "    # Convert query to text embedding\n",
    "    query_embedding = text_model.encode(query).reshape(1, -1)\n",
    "    query_embedding /= np.linalg.norm(query_embedding)\n",
    "\n",
    "    # Search FAISS text database\n",
    "    text_distances, text_results = text_index.search(query_embedding, top_k)\n",
    "    retrieved_pdfs = [(pdf_filenames[idx], text_distances[0][i]) for i, idx in enumerate(text_results[0])]\n",
    "\n",
    "    retrieved_images = []\n",
    "    image_scores = []\n",
    "    unknown_images = []\n",
    "    unknown_pdfs = []\n",
    "\n",
    "    indexed_flowcharts = set(image_filenames)\n",
    "    indexed_pdfs = set(pdf_filenames)\n",
    "\n",
    "    # Handle flowchart images\n",
    "    if flowchart_imgs:\n",
    "        for flowchart_img in flowchart_imgs:\n",
    "            if os.path.basename(flowchart_img) not in indexed_flowcharts:\n",
    "                print(f\"üÜï Marking '{flowchart_img}' as [NEW FLOWCHART] (not in FAISS index)\")\n",
    "                unknown_images.append(flowchart_img)\n",
    "                continue\n",
    "\n",
    "            extracted_text = extract_text_from_image(flowchart_img)\n",
    "            text_embedding = get_text_embedding(extracted_text)\n",
    "\n",
    "            text_distances, text_results = text_index.search(text_embedding.reshape(1, -1), top_k)\n",
    "            retrieved_pdfs += [(pdf_filenames[idx], text_distances[0][i]) for i, idx in enumerate(text_results[0])]\n",
    "\n",
    "            image_query_embedding = get_query_image_embedding(flowchart_img)\n",
    "            image_distances, image_results = image_index.search(image_query_embedding.reshape(1, -1), top_k)\n",
    "\n",
    "            if image_results[0][0] >= 0:\n",
    "                retrieved_images += [image_filenames[idx] for idx in image_results[0]]\n",
    "                image_scores += list(image_distances[0])\n",
    "            else:\n",
    "                unknown_images.append(flowchart_img)\n",
    "\n",
    "    # Handle user-provided PDFs\n",
    "    if user_provided_pdfs:\n",
    "        for user_pdf in user_provided_pdfs:\n",
    "            if os.path.basename(user_pdf) not in indexed_pdfs:\n",
    "                print(f\"üÜï Marking '{user_pdf}' as [NEW PDF] (not in FAISS index)\")\n",
    "                unknown_pdfs.append(user_pdf)\n",
    "            else:\n",
    "                retrieved_pdfs.append((os.path.basename(user_pdf), 0))  # Mark as exact match (0 distance)\n",
    "\n",
    "    # Sort PDFs & images by FAISS similarity scores\n",
    "    retrieved_pdfs = sorted(set(retrieved_pdfs), key=lambda x: x[1])[:top_k]\n",
    "    retrieved_pdfs = [pdf for pdf, _ in retrieved_pdfs]\n",
    "\n",
    "    image_sorted = sorted(zip(retrieved_images, image_scores), key=lambda x: x[1])[:top_k]\n",
    "    retrieved_images = [img for img, _ in image_sorted]\n",
    "\n",
    "    retrieved_images += [f\"[NEW FLOWCHART] {img}\" for img in unknown_images]\n",
    "    retrieved_pdfs += [f\"[NEW PDF] {pdf}\" for pdf in unknown_pdfs]\n",
    "\n",
    "    return retrieved_pdfs, retrieved_images\n",
    "\n",
    "\n",
    "\n",
    "print(\"‚úÖ Retrieval function V2 (With user PDF) is ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd22c25b-b1a2-459b-926b-60adfc08d7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9d74cc14d9f45389a12a5a37ba6c735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n",
    "\n",
    "# Load Qwen model\n",
    "qwen_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\", \n",
    "    torch_dtype=torch.bfloat16,  \n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "qwen_processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cee3aa1f-4093-4a85-bf0b-08e159d6f64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Qwen RAG V2 (with user-provided PDFs) is ready.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def query_qwen_with_rag(query, flowchart_imgs=None, user_provided_pdfs=None, top_k=1):\n",
    "    # Work related keywords are here so that the model don't automatically fetch documents for prompts that has nothing to do with work.\n",
    "    # Feel free to comment this out should it be needed.\n",
    "    work_related_keywords = [\n",
    "        \"flowchart\", \"document\", \"database\", \"pdf\", \"incident\",\n",
    "        \"management\", \"network\", \"troubleshooting\", \"cybersecurity\",\n",
    "        \"procedure\", \"process\", \"policy\", \"workflow\", \"information\"\n",
    "    ]\n",
    "\n",
    "    # Check if query is work-related\n",
    "    if not any(keyword in query.lower() for keyword in work_related_keywords):\n",
    "        # If not work-related, skip retrieval and context\n",
    "        text_input = qwen_processor.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": query}]}],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        inputs = qwen_processor(\n",
    "            text=[text_input],\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(qwen_model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = qwen_model.generate(**inputs, max_new_tokens=1024)\n",
    "\n",
    "        response_text = qwen_processor.batch_decode(output_ids, skip_special_tokens=False)[0]\n",
    "\n",
    "        match = re.search(r\"assistant\\s*\\n(.*)\", response_text, re.DOTALL)\n",
    "        cleaned_response = match.group(1).strip() if match else response_text.strip()\n",
    "\n",
    "        return cleaned_response, [], []\n",
    "\n",
    "    # Otherwise, proceed normally\n",
    "    if flowchart_imgs and isinstance(flowchart_imgs, str):\n",
    "        flowchart_imgs = [flowchart_imgs]\n",
    "\n",
    "    if user_provided_pdfs and isinstance(user_provided_pdfs, str):\n",
    "        user_provided_pdfs = [user_provided_pdfs]\n",
    "\n",
    "    retrieved_pdfs, retrieved_images = retrieve_relevant_data(\n",
    "        query=query,\n",
    "        flowchart_imgs=flowchart_imgs,\n",
    "        user_provided_pdfs=user_provided_pdfs,\n",
    "        top_k=top_k\n",
    "    )\n",
    "\n",
    "    # Extract text from retrieved PDFs, ignoring \"[NEW PDF]\" entries\n",
    "    context = \"\\n\".join([pdf_texts[pdf] for pdf in retrieved_pdfs if not pdf.startswith(\"[NEW PDF]\")])\n",
    "\n",
    "    # Extract and explicitly include text from user-provided PDFs\n",
    "    user_pdf_texts = []\n",
    "    descriptions = []\n",
    "    \n",
    "    if user_provided_pdfs:\n",
    "        for user_pdf in user_provided_pdfs:\n",
    "            is_new_pdf = os.path.basename(user_pdf) not in pdf_filenames\n",
    "            \n",
    "            if is_new_pdf:\n",
    "                descriptions.append(f\"üÜï This is a newly provided PDF: {os.path.basename(user_pdf)}\")\n",
    "                extracted_text = extract_text_from_pdf(user_pdf)\n",
    "                user_pdf_texts.append(\n",
    "                    f\"[NEW PDF] User-Provided PDF '{os.path.basename(user_pdf)}':\\n\\n{extracted_text}\\n\\n\"\n",
    "                    \"[NOTE: This PDF is explicitly NOT part of the existing database.]\"\n",
    "                )\n",
    "            else:\n",
    "                extracted_text = pdf_texts[os.path.basename(user_pdf)]\n",
    "                user_pdf_texts.append(f\"User-Provided PDF '{os.path.basename(user_pdf)}':\\n\\n{extracted_text}\")\n",
    "    \n",
    "    user_pdf_section = \"\\n\\n\".join(user_pdf_texts)\n",
    "\n",
    "\n",
    "    # Extract flowchart texts (unchanged)\n",
    "    flowchart_texts = []\n",
    "    valid_images = []\n",
    "    \n",
    "    if flowchart_imgs:\n",
    "        for img_path in flowchart_imgs:\n",
    "            is_new_flowchart = os.path.basename(img_path) not in image_filenames\n",
    "            \n",
    "            if is_new_flowchart:\n",
    "                descriptions.append(f\"üÜï This is a newly provided flowchart: {os.path.basename(img_path)}\")\n",
    "                extracted_text = extract_text_from_image(img_path)\n",
    "                flowchart_texts.append(\n",
    "                    f\"[NEW FLOWCHART] Flowchart {len(flowchart_texts) + 1} (User-Provided):\\n\\n{extracted_text}\\n\\n\"\n",
    "                    \"[NOTE: This flowchart is explicitly NOT part of the existing database.]\"\n",
    "                )\n",
    "            else:\n",
    "                extracted_text = extract_text_from_image(img_path)\n",
    "                flowchart_texts.append(f\"Flowchart {len(flowchart_texts) + 1}:\\n\\n{extracted_text}\")\n",
    "            \n",
    "            valid_images.append(Image.open(img_path).convert(\"RGB\"))\n",
    "    \n",
    "    flowchart_section = \"\\n\\n\".join(flowchart_texts)\n",
    "\n",
    "\n",
    "    # PROMPT\n",
    "    content = [\n",
    "        *([{\"type\": \"image\", \"image\": img} for img in valid_images]),\n",
    "        *([{\"type\": \"text\", \"text\": desc} for desc in descriptions]),\n",
    "        {\"type\": \"text\", \"text\": (\n",
    "            \"Carefully review the provided context below:\\n\\n\"\n",
    "            f\"Indexed Documents:\\n{context}\\n\\n\"\n",
    "            f\"User-Provided PDF Context:\\n{user_pdf_section}\\n\\n\"\n",
    "            f\"Flowchart Information:\\n{flowchart_section}\\n\\n\"\n",
    "            \"Important clarifications:\\n\"\n",
    "            \"- Documents labeled '[NEW PDF]' or '[NEW FLOWCHART]' are NOT part of the database.\\n\"\n",
    "            \"- Documents without these labels ARE part of the database.\\n\\n\"\n",
    "            \"Your task:\\n\"\n",
    "            \"1. Clearly state whether the relevant document or flowchart is currently in the database.\\n\"\n",
    "            \"2. Provide a summary explaining its key contents and purpose, explicitly referencing the context provided. Pay attention if the user wants it brief or detailed.\\n\\n\"\n",
    "            f\"Query: {query}\"\n",
    "        )},\n",
    "    ]\n",
    "\n",
    "    # Prepare Qwen inputs as before\n",
    "    if not valid_images:\n",
    "        text_input = qwen_processor.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": content}], \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        inputs = qwen_processor(\n",
    "            text=[text_input],\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(qwen_model.device)\n",
    "    else:\n",
    "        text_input = qwen_processor.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": content}], \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        inputs = qwen_processor(\n",
    "            text=[text_input],\n",
    "            images=valid_images,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(qwen_model.device)\n",
    "\n",
    "    if not hasattr(inputs, \"input_ids\"):\n",
    "        print(\"‚ö†Ô∏è Error: Inputs are incorrectly formatted. Skipping generation.\")\n",
    "        return \"Error: Invalid input formatting\", retrieved_pdfs, retrieved_images\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = qwen_model.generate(**inputs, max_new_tokens=1024)\n",
    "\n",
    "    response_text = qwen_processor.batch_decode(output_ids, skip_special_tokens=False)[0]\n",
    "\n",
    "    match = re.search(r\"assistant\\s*\\n(.*)\", response_text, re.DOTALL)\n",
    "    cleaned_response = match.group(1).strip() if match else response_text.strip()\n",
    "\n",
    "    return cleaned_response, retrieved_pdfs, retrieved_images\n",
    "\n",
    "\n",
    "print(\"‚úÖ Qwen RAG V2 (with user-provided PDFs) is ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44577d78-b7c8-4ebc-8a1e-564bb8bfdb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Qwen's Response (With RAG):\n",
      "Hello! How can I assist you today?<|im_end|>\n",
      "üîç Retrieved Documents: []\n",
      "üñºÔ∏è Retrieved Flowcharts: []\n",
      "‚è≥ Execution Time: 0.32 seconds\n",
      "\n",
      "ü§ñ Qwen's Response (With RAG):\n",
      "### Relevant Document or Flowchart in the Database\n",
      "\n",
      "**Document Name:** Incident Management Flowchart\n",
      "\n",
      "### Summary of Key Contents and Purpose\n",
      "\n",
      "The Incident Management Flowchart outlines a structured approach to handling IT issues efficiently. It includes the following key steps:\n",
      "\n",
      "1. **User Reports Issue**: The process begins when a user identifies an issue and reports it to the IT department through various channels such as a ticketing system, phone call, or automated monitoring alerts.\n",
      "\n",
      "2. **Classify Incident**: The support team categorizes the issue based on its nature, urgency, and complexity to determine the appropriate response.\n",
      "\n",
      "3. **High Impact?**: The severity of the incident is evaluated. If it is a high-impact issue affecting multiple users or critical systems, it is escalated immediately.\n",
      "\n",
      "4. **Assign to Support Team**: If the issue is not high impact, it is assigned to the appropriate support team or technician for further investigation and resolution.\n",
      "\n",
      "5. **Resolve Issue**: The assigned technician follows predefined troubleshooting steps to identify and fix the problem.\n",
      "\n",
      "6. **Issue Resolved?**: If the issue is resolved, the ticket is closed. If not, further escalation is required.\n",
      "\n",
      "7. **Escalate to Higher Support**: If the support team cannot resolve the issue, it is escalated to a senior technician or a specialized support team for advanced troubleshooting.\n",
      "\n",
      "8. **Close Ticket**: Once the issue is successfully resolved, the support team confirms with the user and closes the ticket in the system, ensuring proper documentation.\n",
      "\n",
      "This flowchart aims to reduce downtime, minimize disruptions, and ensure smooth operations by providing a standardized incident response workflow.<|im_end|>\n",
      "üîç Retrieved Documents: ['6_Incident_Management.pdf']\n",
      "üñºÔ∏è Retrieved Flowcharts: []\n",
      "‚è≥ Execution Time: 5.61 seconds\n"
     ]
    }
   ],
   "source": [
    "# PROMPT\n",
    "\n",
    "import time\n",
    "\n",
    "user_query = \"Hello there!\"\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Query Qwen with RAG\n",
    "qwen_response_rag, retrieved_pdfs, retrieved_images = query_qwen_with_rag(user_query)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nü§ñ Qwen's Response (With RAG):\\n{qwen_response_rag}\")\n",
    "print(f\"üîç Retrieved Documents: {retrieved_pdfs}\")\n",
    "print(f\"üñºÔ∏è Retrieved Flowcharts: {retrieved_images}\")\n",
    "print(f\"‚è≥ Execution Time: {execution_time:.2f} seconds\")\n",
    "\n",
    "# user_query_2 = \"What is your function?\"\n",
    "user_query_2 = \"Can you give me information about handling incident management?\"\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Query Qwen with RAG\n",
    "qwen_response_rag, retrieved_pdfs, retrieved_images = query_qwen_with_rag(user_query_2)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nü§ñ Qwen's Response (With RAG):\\n{qwen_response_rag}\")\n",
    "print(f\"üîç Retrieved Documents: {retrieved_pdfs}\")\n",
    "print(f\"üñºÔ∏è Retrieved Flowcharts: {retrieved_images}\")\n",
    "print(f\"‚è≥ Execution Time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddbb66fa-ca55-4d54-af4a-324be153c47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Qwen's Response (With RAG):\n",
      "This flowchart outlines a structured approach to machine maintenance and troubleshooting within an industrial setting. It begins with the identification of a machine issue through formal reporting by operators or monitoring systems. The next step involves diagnosing the problem to determine the root cause. Depending on whether the issue is minor or requires further investigation, different actions are taken. For minor issues, quick fixes such as recalibration or replacing minor parts are performed. If the issue cannot be resolved quickly, the machine is tested under operational conditions to ensure the problem has been fully addressed. If the issue persists, a major repair is scheduled, which involves taking the machine offline for comprehensive repairs or component replacements. Once the repairs are completed and verified, the machine returns to operational status. This flowchart serves as a guide for technical teams to systematically detect and resolve malfunctions, thereby minimizing downtime, reducing repair costs, and extending the lifespan of valuable equipment.<|im_end|>\n",
      "üîç Retrieved Documents: ['13_Machine_Maintenance.pdf']\n",
      "üñºÔ∏è Retrieved Flowcharts: ['13_machine_maintenance.png']\n",
      "‚è≥ Execution Time: 3.84 seconds\n"
     ]
    }
   ],
   "source": [
    "# IMAGE + PROMPT\n",
    "\n",
    "import time\n",
    "\n",
    "# Define the test query and flowchart image\n",
    "user_query = \"Explain what this flowchart is about in detail.\"\n",
    "\n",
    "user_flowchart_img = \"business_flowcharts/flowcharts/13_machine_maintenance.png\"\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Query Qwen with RAG\n",
    "qwen_response_rag, retrieved_pdfs, retrieved_images = query_qwen_with_rag(user_query, user_flowchart_img)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nü§ñ Qwen's Response (With RAG):\\n{qwen_response_rag}\")\n",
    "print(f\"üîç Retrieved Documents: {retrieved_pdfs}\")\n",
    "print(f\"üñºÔ∏è Retrieved Flowcharts: {retrieved_images}\")\n",
    "print(f\"‚è≥ Execution Time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b43813de-cd21-4e02-b106-aa6211d10d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üÜï Marking '6_incident_management_v2.png' as [NEW FLOWCHART] (not in FAISS index)\n",
      "üÜï Marking '6_Incident_Management_V2.pdf' as [NEW PDF] (not in FAISS index)\n",
      "\n",
      "ü§ñ Qwen's Response (With RAG):\n",
      "### Summary of the New Incident Management Process\n",
      "\n",
      "#### Key Contents:\n",
      "1. **Start**: The incident management process begins when an issue is reported or detected.\n",
      "2. **User Reports Issue**: A user experiences a problem and submits an incident report. Details like error messages, timestamps, and impact assessments are crucial for efficient resolution.\n",
      "3. **Initial Triage**: The support team performs an initial assessment to determine the urgency, scope, and potential resolution path.\n",
      "4. **Classify Incident**: The incident is categorized based on its nature (e.g., hardware failure, software bug, network issue). Proper classification ensures the correct resolution protocol is followed.\n",
      "5. **Automated Resolution?**: A decision is made regarding whether the issue can be resolved automatically. Common automated solutions include system reboots, automated scripts for patching, or self-healing mechanisms.\n",
      "6. **High Impact?**: The impact of the incident is assessed. If the issue affects critical business functions, large user groups, or essential services, it is classified as 'High Impact,' requiring priority handling.\n",
      "7. **Critical?**: For high-impact issues, an additional check determines whether the incident is critical. A 'Critical' classification is given if the issue threatens system security, data integrity, or core infrastructure.\n",
      "8. **Assign to Support Team**: If the incident is neither critical nor resolvable via automation, it is assigned to the appropriate support team. The team analyzes logs, reproduces the issue if necessary, and starts working on a resolution.\n",
      "9. **Resolve Issue**: The support team applies fixes, patches, or configuration changes to resolve the incident. This may involve collaboration with external vendors, development teams, or database administrators.\n",
      "10. **Issue Resolved?**: A verification step is conducted to determine whether the applied fix has successfully resolved the problem. This includes user confirmation, system tests, and log analysis.\n",
      "11. **Follow-up Needed?**: If the issue is not fully resolved or requires monitoring, a follow-up process is initiated. This step ensures that similar incidents do not reoccur and that the fix is sustainable.\n",
      "12. **Escalate to Higher Support**: If the issue is unresolved or requires higher expertise, it is escalated to a senior technical team, specialized engineers, or external support.\n",
      "13. **Close Ticket**: Once the issue is confirmed to be resolved and any necessary follow-ups are completed, the ticket is officially closed. Documentation of the incident and resolution steps is logged for future reference and knowledge base updates.\n",
      "\n",
      "#### Purpose:\n",
      "The new incident management process aims to improve the efficiency and effectiveness of handling reported issues by providing a structured workflow that includes automated resolution pathways, impact classification, and follow-up mechanisms. This ensures that resources are allocated appropriately, and issues are resolved promptly and efficiently.\n",
      "\n",
      "### Comparison with Existing Database:\n",
      "The new incident management process appears to be an improvement over the existing one in several ways:\n",
      "- **Enhanced Automation**: The inclusion of automated resolution pathways suggests a more proactive approach to handling routine issues.\n",
      "- **Impact Classification**: The addition of impact classification helps prioritize critical issues, ensuring they receive immediate attention.\n",
      "- **Detailed Follow-Up Mechanism**: The introduction of a follow-up process after resolution ensures that issues are thoroughly addressed and prevent similar problems from recurring.\n",
      "- **Escalation Handling**: The escalation mechanism for complex or unresolved issues provides a clear pathway for advanced troubleshooting, ensuring that persistent or complex incidents receive additional scrutiny.\n",
      "\n",
      "Overall, the new process seems to offer a more comprehensive and efficient approach to incident management, making it a significant enhancement over the existing workflow.<|im_end|>\n",
      "üîç Retrieved Documents: ['6_Incident_Management.pdf', '[NEW PDF] 6_Incident_Management_V2.pdf']\n",
      "üñºÔ∏è Retrieved Flowcharts: ['[NEW FLOWCHART] 6_incident_management_v2.png']\n",
      "‚è≥ Execution Time: 12.81 seconds\n"
     ]
    }
   ],
   "source": [
    "# IMAGE + PDF + PROMPT\n",
    "\n",
    "import time\n",
    "\n",
    "# Define the test query and flowchart image\n",
    "user_query = \"Check this new incident management flowchart and pdf I provided. Is it an improvement from the one saved in the database?\"\n",
    "\n",
    "user_flowchart_img = \"6_incident_management_v2.png\"\n",
    "user_pdf = \"6_Incident_Management_V2.pdf\"\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Query Qwen with RAG\n",
    "qwen_response_rag, retrieved_pdfs, retrieved_images = query_qwen_with_rag(user_query, user_flowchart_img, user_pdf)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nü§ñ Qwen's Response (With RAG):\\n{qwen_response_rag}\")\n",
    "print(f\"üîç Retrieved Documents: {retrieved_pdfs}\")\n",
    "print(f\"üñºÔ∏è Retrieved Flowcharts: {retrieved_images}\")\n",
    "print(f\"‚è≥ Execution Time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4785823-249e-46b7-b097-24ff2b1e802b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üÜï Marking '6_Incident_Management_V2.pdf' as [NEW PDF] (not in FAISS index)\n",
      "\n",
      "ü§ñ Qwen's Response (With RAG):\n",
      "### 1. Is this incident management PDF document an improvement from the one saved in the database?\n",
      "\n",
      "**Yes, this incident management PDF document is an improvement from the one saved in the database.**\n",
      "\n",
      "**Explanation:**\n",
      "The new document, \"6_Incident_Management_V2.pdf,\" provides a more detailed and structured approach to incident management compared to the previous version saved in the database. Here are the key improvements and additions:\n",
      "\n",
      "- **Enhanced Workflow:** The new document includes a more comprehensive flowchart that outlines the entire incident management process, from reporting to resolution and escalation.\n",
      "  \n",
      "- **Structured Triage:** It introduces an \"Initial Triage\" step, which helps in quickly assessing the urgency and scope of the incident, allowing for more efficient resource allocation.\n",
      "\n",
      "- **Automated Resolution Pathways:** The document specifies how to determine if an issue can be resolved automatically, using common automated solutions like system reboots, automated scripts, or self-healing mechanisms.\n",
      "\n",
      "- **Impact Classification:** It includes a detailed classification process, distinguishing between high-impact and critical issues, which ensures that high-impact problems receive immediate attention.\n",
      "\n",
      "- **Follow-Up Mechanisms:** The new document emphasizes the importance of follow-up processes to ensure that issues are fully resolved and prevent similar incidents from recurring.\n",
      "\n",
      "- **Escalation Procedures:** It outlines clear escalation procedures, ensuring that complex or unresolved issues are escalated to higher levels of support when necessary.\n",
      "\n",
      "- **Documentation:** The document stresses the importance of documenting the incident and resolution steps, which is crucial for future reference and knowledge base updates.\n",
      "\n",
      "Overall, the new document offers a more robust and structured approach to incident management, addressing many of the shortcomings of the previous version while incorporating modern best practices and enhancements.<|im_end|>\n",
      "üîç Retrieved Documents: ['6_Incident_Management.pdf', '[NEW PDF] 6_Incident_Management_V2.pdf']\n",
      "üñºÔ∏è Retrieved Flowcharts: []\n",
      "‚è≥ Execution Time: 6.14 seconds\n"
     ]
    }
   ],
   "source": [
    "# PDF + PROMPT\n",
    "\n",
    "import time\n",
    "\n",
    "# Define the test query and flowchart image\n",
    "user_query = \"Is this incident management pdf document an improvement from the one saved in the database? Explain in detail\"\n",
    "user_flowchart_img = []\n",
    "user_pdf = \"6_Incident_Management_V2.pdf\"\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Query Qwen with RAG\n",
    "qwen_response_rag, retrieved_pdfs, retrieved_images = query_qwen_with_rag(user_query, user_flowchart_img, user_pdf)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nü§ñ Qwen's Response (With RAG):\\n{qwen_response_rag}\")\n",
    "print(f\"üîç Retrieved Documents: {retrieved_pdfs}\")\n",
    "print(f\"üñºÔ∏è Retrieved Flowcharts: {retrieved_images}\")\n",
    "print(f\"‚è≥ Execution Time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "342381d4-89c9-4fc7-8898-f38cc5c49774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üÜï Marking '6_Incident_Management_V2.pdf' as [NEW PDF] (not in FAISS index)\n",
      "\n",
      "ü§ñ Qwen's Response (With RAG):\n",
      "### Summary of the New Incident Management PDF\n",
      "\n",
      "**Document Title:** Incident Management Process Flowchart (V2)\n",
      "\n",
      "**Purpose:** \n",
      "This document provides an improved incident management process aimed at enhancing the efficiency and effectiveness of resolving reported issues. It includes structured triage, automated resolution pathways, impact classification, and follow-up mechanisms to ensure that resources are allocated appropriately and that the appropriate teams are assigned for resolution.\n",
      "\n",
      "**Key Contents:**\n",
      "\n",
      "1. **Start:**\n",
      "   - The incident management process begins when an issue is reported or detected, triggering the workflow.\n",
      "\n",
      "2. **User Reports Issue:**\n",
      "   - Users submit incident reports through various channels like ticketing systems, emails, or direct calls to the support team. The details provided by the user, including error messages, timestamps, and impact assessments, are crucial for efficient resolution.\n",
      "\n",
      "3. **Initial Triage:**\n",
      "   - The support team performs an initial assessment of the incident to determine its urgency, scope, and potential resolution path. This step helps filter simple issues from complex ones, ensuring that resources are allocated appropriately.\n",
      "\n",
      "4. **Classify Incident:**\n",
      "   - The incident is categorized based on its nature (e.g., hardware failure, software bug, network issue). Proper classification ensures that the correct resolution protocol is followed and that the appropriate team is assigned.\n",
      "\n",
      "5. **Automated Resolution?**\n",
      "   - A decision is made regarding whether the issue can be resolved automatically. Common automated solutions include system reboots, automated scripts for patching, or self-healing mechanisms. If automation is possible, the process moves directly to closing the ticket.\n",
      "\n",
      "6. **High Impact?**\n",
      "   - The impact of the incident is assessed. If the issue affects critical business functions, large user groups, or essential services, it is classified as 'High Impact,' requiring priority handling.\n",
      "\n",
      "7. **Critical?**\n",
      "   - For high-impact issues, an additional check determines whether the incident is critical. A 'Critical' classification is given if the issue threatens system security, data integrity, or core infrastructure. If critical, the incident is escalated immediately.\n",
      "\n",
      "8. **Assign to Support Team:**\n",
      "   - If the incident is neither critical nor resolvable via automation, it is assigned to the appropriate support team. The team members analyze logs, reproduce the issue if necessary, and start working on a resolution.\n",
      "\n",
      "9. **Resolve Issue:**\n",
      "   - The support team applies fixes, patches, or configuration changes to resolve the incident. This may involve collaboration with external vendors, development teams, or database administrators to implement a permanent solution.\n",
      "\n",
      "10. **Issue Resolved?**\n",
      "    - A verification step is conducted to determine whether the applied fix has successfully resolved the problem. This includes user confirmation, system tests, and log analysis.\n",
      "\n",
      "11. **Follow-up Needed?**\n",
      "    - If the issue is not fully resolved or requires monitoring, a follow-up process is initiated. This step ensures that similar incidents do not reoccur and that the fix is sustainable.\n",
      "\n",
      "12. **Escalate to Higher Support:**\n",
      "    - If the issue is unresolved or requires higher expertise, it is escalated to a senior technical team, specialized engineers, or external support. Escalation ensures that persistent or complex incidents receive additional scrutiny.\n",
      "\n",
      "13. **Close Ticket:**\n",
      "    - Once the issue is confirmed to be resolved and any necessary follow-ups are completed, the ticket is officially closed. Documentation of the incident and resolution steps is logged for future reference and knowledge base updates.\n",
      "\n",
      "### Comparison with the Old Incident Management PDF\n",
      "\n",
      "While both documents aim to improve incident management processes, the new PDF (6_Incident_Management_V2.pdf) offers several enhancements over the old one:\n",
      "\n",
      "1. **Structured Triage:** The new document includes an initial assessment step to determine the urgency and scope of the incident, which helps in filtering simple issues from complex ones more effectively.\n",
      "   \n",
      "2. **Automated Resolution Pathways:** The document introduces automated solutions for common issues, reducing manual intervention and speeding up resolution times.\n",
      "\n",
      "3. **Impact Classification:** The new document emphasizes the importance of impact classification, ensuring that high-impact issues are handled promptly and prioritized accordingly.\n",
      "\n",
      "4. **Follow-Up Mechanisms:** The inclusion of a follow-up process ensures that similar incidents do not reoccur and that the fix is sustainable, improving long-term operational efficiency.\n",
      "\n",
      "5. **Detailed Escalation Handling:** The new document provides a more detailed escalation process, ensuring that complex or persistent issues receive thorough scrutiny from senior technical teams or specialized engineers.\n",
      "\n",
      "Overall, the new PDF appears to offer a more comprehensive and structured approach to incident management, with enhanced features for automated resolution, impact classification, and follow-up mechanisms. These improvements likely contribute to faster resolution times and more efficient use of resources.<|im_end|>\n",
      "üîç Retrieved Documents: ['6_Incident_Management.pdf', '[NEW PDF] 6_Incident_Management_V2.pdf']\n",
      "üñºÔ∏è Retrieved Flowcharts: []\n",
      "‚è≥ Execution Time: 16.56 seconds\n"
     ]
    }
   ],
   "source": [
    "# PDF + PROMPT\n",
    "\n",
    "import time\n",
    "\n",
    "# Define the test query and flowchart image\n",
    "user_query = \"Read out this pdf. Is it better than the old incident management pdf?\"\n",
    "user_flowchart_img = []\n",
    "user_pdf = \"6_Incident_Management_V2.pdf\"\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Query Qwen with RAG\n",
    "qwen_response_rag, retrieved_pdfs, retrieved_images = query_qwen_with_rag(user_query, user_flowchart_img, user_pdf)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nü§ñ Qwen's Response (With RAG):\\n{qwen_response_rag}\")\n",
    "print(f\"üîç Retrieved Documents: {retrieved_pdfs}\")\n",
    "print(f\"üñºÔ∏è Retrieved Flowcharts: {retrieved_images}\")\n",
    "print(f\"‚è≥ Execution Time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bee64c3f-82d7-479a-aa95-a604f4d06471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Improved Comparison RAG is set!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def compare_query_qwen_with_rag(query, flowchart_imgs=None, user_provided_pdfs=None, top_k=1):\n",
    "    \"\"\"Retrieve relevant document and flowchart data, and query Qwen for a comparison-focused response.\"\"\"\n",
    "\n",
    "    # Ensure inputs are in list format\n",
    "    if flowchart_imgs and isinstance(flowchart_imgs, str):\n",
    "        flowchart_imgs = [flowchart_imgs]\n",
    "    if user_provided_pdfs and isinstance(user_provided_pdfs, str):\n",
    "        user_provided_pdfs = [user_provided_pdfs]\n",
    "\n",
    "    # Retrieve relevant PDFs and flowcharts\n",
    "    retrieved_pdfs, retrieved_images = retrieve_relevant_data(\n",
    "        query=query,\n",
    "        flowchart_imgs=flowchart_imgs,\n",
    "        user_provided_pdfs=user_provided_pdfs,\n",
    "        top_k=top_k\n",
    "    )\n",
    "\n",
    "    # Extract context from indexed PDFs (exclude [NEW PDF] labels)\n",
    "    context = \"\\n\".join([pdf_texts[pdf] for pdf in retrieved_pdfs if not pdf.startswith(\"[NEW PDF]\")])\n",
    "\n",
    "    # Extract user-provided PDF text\n",
    "    user_pdf_texts = []\n",
    "    descriptions = []\n",
    "\n",
    "    if user_provided_pdfs:\n",
    "        for pdf_path in user_provided_pdfs:\n",
    "            if not os.path.exists(pdf_path):\n",
    "                print(f\"‚ö†Ô∏è Warning: PDF '{pdf_path}' not found. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            is_new_pdf = os.path.basename(pdf_path) not in pdf_filenames\n",
    "            extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "            label = \"[NEW PDF] \" if is_new_pdf else \"\"\n",
    "            note = \"\\n[NOTE: This PDF is not in the existing database.]\" if is_new_pdf else \"\"\n",
    "            user_pdf_texts.append(\n",
    "                f\"{label}User-Provided PDF '{os.path.basename(pdf_path)}':\\n\\n{extracted_text}{note}\"\n",
    "            )\n",
    "            if is_new_pdf:\n",
    "                descriptions.append(f\"üÜï This is a newly provided PDF: {os.path.basename(pdf_path)}\")\n",
    "\n",
    "    user_pdf_section = \"\\n\\n\".join(user_pdf_texts)\n",
    "\n",
    "    # Extract flowchart OCR text\n",
    "    flowchart_texts = []\n",
    "    valid_images = []\n",
    "\n",
    "    if flowchart_imgs:\n",
    "        for img_path in flowchart_imgs:\n",
    "            is_new_flowchart = os.path.basename(img_path) not in image_filenames\n",
    "\n",
    "            if not os.path.exists(img_path):\n",
    "                print(f\"‚ö†Ô∏è Warning: Image '{img_path}' not found. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            if is_new_flowchart:\n",
    "                descriptions.append(f\"üÜï This is a newly provided flowchart: {os.path.basename(img_path)}\")\n",
    "                extracted_text = extract_text_from_image(img_path)\n",
    "                flowchart_texts.append(\n",
    "                    f\"[NEW FLOWCHART] Flowchart {len(flowchart_texts) + 1} (User-Provided):\\n\\n{extracted_text}\\n\\n\"\n",
    "                    \"[NOTE: This flowchart is NOT in the existing database.]\"\n",
    "                )\n",
    "            else:\n",
    "                extracted_text = extract_text_from_image(img_path)\n",
    "                flowchart_texts.append(f\"Flowchart {len(flowchart_texts) + 1}:\\n\\n{extracted_text}\")\n",
    "\n",
    "            valid_images.append(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "    flowchart_section = \"\\n\\n\".join(flowchart_texts)\n",
    "\n",
    "    # PROMPT\n",
    "    content = [\n",
    "        *([{\"type\": \"image\", \"image\": img} for img in valid_images]),\n",
    "        *([{\"type\": \"text\", \"text\": desc} for desc in descriptions]),\n",
    "        {\"type\": \"text\", \"text\": (\n",
    "            \"You are provided with the following information for comparison:\\n\\n\"\n",
    "    \n",
    "            \"### SECTION 1: Content from the Existing Database\\n\"\n",
    "            \"**Database PDFs:**\\n\"\n",
    "            f\"{context if context.strip() else '[No indexed documents retrieved]'}\\n\\n\"\n",
    "    \n",
    "            \"**Database Flowcharts (OCR Extracted Text):**\\n\"\n",
    "            f\"{flowchart_section if flowchart_section.strip() else '[No indexed flowcharts retrieved]'}\\n\\n\"\n",
    "    \n",
    "            \"### SECTION 2: User-Provided Content for Comparison\\n\"\n",
    "            \"**User PDFs:**\\n\"\n",
    "            f\"{user_pdf_section if user_pdf_section.strip() else '[No user PDFs provided]'}\\n\\n\"\n",
    "    \n",
    "            \"**User Flowcharts (OCR Extracted Text):**\\n\"\n",
    "            f\"{flowchart_section if flowchart_section.strip() else '[No user flowcharts provided]'}\\n\\n\"\n",
    "    \n",
    "            \"### Task:\\n\"\n",
    "            \"- Carefully compare the content in Section 2 (user-provided) against Section 1 (database).\\n\"\n",
    "            \"- If multiple flowcharts are provided, **compare the second one to the first**.\\n\"\n",
    "            \"- Highlight step-by-step differences, changes in structure, terminology, or flow.\\n\"\n",
    "            \"- Summarize each major difference clearly.\\n\\n\"\n",
    "            f\"{query}\"\n",
    "        )}\n",
    "    ]\n",
    "\n",
    "    # Handle text-only queries if no images are involved\n",
    "    if not valid_images:\n",
    "        print(\"üîπ No images detected, processing as a pure text query.\")\n",
    "        text_input = qwen_processor.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": content}],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        inputs = qwen_processor(\n",
    "            text=[text_input],\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(qwen_model.device)\n",
    "    else:\n",
    "        text_input = qwen_processor.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": content}],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        inputs = qwen_processor(\n",
    "            text=[text_input],\n",
    "            images=valid_images,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(qwen_model.device)\n",
    "\n",
    "    # Sanity check before generating\n",
    "    if not hasattr(inputs, \"input_ids\"):\n",
    "        print(\"‚ö†Ô∏è Error: Inputs are incorrectly formatted. Skipping generation.\")\n",
    "        return \"Error: Invalid input formatting\", retrieved_pdfs, retrieved_images\n",
    "\n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        output_ids = qwen_model.generate(**inputs, max_new_tokens=1024)\n",
    "\n",
    "    # Decode and clean response\n",
    "    response_text = qwen_processor.batch_decode(output_ids, skip_special_tokens=False)[0]\n",
    "    match = re.search(r\"assistant\\s*\\n(.*)\", response_text, re.DOTALL)\n",
    "    cleaned_response = match.group(1).strip() if match else response_text.strip()\n",
    "\n",
    "    return cleaned_response, retrieved_pdfs, retrieved_images\n",
    "\n",
    "print(\"‚úÖ Improved Comparison RAG is set!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "842614af-fc72-404b-8bbe-072c7bc1db2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üÜï Marking '6_incident_management_v2.png' as [NEW FLOWCHART] (not in FAISS index)\n",
      "üÜï Marking '6_Incident_Management_V2.pdf' as [NEW PDF] (not in FAISS index)\n",
      "\n",
      "ü§ñ Qwen's Response (With RAG):\n",
      "Certainly! Let's carefully compare the content in Section 2 (user-provided) against Section 1 (database), focusing on the differences, changes in structure, terminology, or flow.\n",
      "\n",
      "### Differences Between the First and Second Flowcharts\n",
      "\n",
      "#### 1. **Initial Triage**\n",
      "   - **Database Flowchart:** No specific mention of \"Initial Triage.\"\n",
      "   - **User Provided Flowchart:** Added \"Initial Triage\" as a step after \"User Reports Issue.\"\n",
      "\n",
      "#### 2. **Automated Resolution?**\n",
      "   - **Database Flowchart:** No specific mention of \"Automated Resolution?\"\n",
      "   - **User Provided Flowchart:** Added \"Automated Resolution?\" as a step after \"Classify Incident.\"\n",
      "\n",
      "#### 3. **High Impact?**\n",
      "   - **Database Flowchart:** No specific mention of \"High Impact?\"\n",
      "   - **User Provided Flowchart:** Added \"High Impact?\" as a step after \"Classify Incident.\"\n",
      "\n",
      "#### 4. **Critical?**\n",
      "   - **Database Flowchart:** No specific mention of \"Critical?\"\n",
      "   - **User Provided Flowchart:** Added \"Critical?\" as a step after \"High Impact?\"\n",
      "\n",
      "#### 5. **Follow-up Needed?**\n",
      "   - **Database Flowchart:** No specific mention of \"Follow-up Needed?\"\n",
      "   - **User Provided Flowchart:** Added \"Follow-up Needed?\" as a step after \"Issue Resolved?\"\n",
      "\n",
      "#### 6. **Escalate to Higher Support**\n",
      "   - **Database Flowchart:** No specific mention of \"Escalate to Higher Support.\"\n",
      "   - **User Provided Flowchart:** Added \"Escalate to Higher Support\" as a step after \"Issue Resolved?\"\n",
      "\n",
      "### Summary of Differences\n",
      "\n",
      "1. **Initial Triage:**\n",
      "   - The user-provided flowchart includes an \"Initial Triage\" step, which is absent in the database flowchart.\n",
      "\n",
      "2. **Automated Resolution?**\n",
      "   - The user-provided flowchart includes an \"Automated Resolution?\" step, which is absent in the database flowchart.\n",
      "\n",
      "3. **High Impact?**\n",
      "   - The user-provided flowchart includes a \"High Impact?\" step, which is absent in the database flowchart.\n",
      "\n",
      "4. **Critical?**\n",
      "   - The user-provided flowchart includes a \"Critical?\" step, which is absent in the database flowchart.\n",
      "\n",
      "5. **Follow-up Needed?**\n",
      "   - The user-provided flowchart includes a \"Follow-up Needed?\" step, which is absent in the database flowchart.\n",
      "\n",
      "6. **Escalate to Higher Support:**\n",
      "   - The user-provided flowchart includes an \"Escalate to Higher Support\" step, which is absent in the database flowchart.\n",
      "\n",
      "These differences highlight the enhancements and additions made by the user in their flowchart compared to the existing database flowchart. The user-provided flowchart appears more comprehensive and detailed, incorporating additional steps and considerations that might be relevant in real-world incident management processes.<|im_end|>\n",
      "üîç Retrieved Documents: ['6_Incident_Management.pdf', '[NEW PDF] 6_Incident_Management_V2.pdf']\n",
      "üñºÔ∏è Retrieved Flowcharts: ['6_incident_management.png', '[NEW FLOWCHART] 6_incident_management_v2.png']\n",
      "‚è≥ Execution Time: 11.10 seconds\n"
     ]
    }
   ],
   "source": [
    "user_query = \"Can you compare these flowcharts and pdfs in detail? What are the differences between the first and the second one?\"\n",
    "\n",
    "flowchart_imgs = [\n",
    "    \"business_flowcharts/flowcharts/6_incident_management.png\",\n",
    "    \"6_incident_management_v2.png\"\n",
    "]\n",
    "\n",
    "# flowchart_imgs = [\n",
    "#     \"business_flowcharts/flowcharts/9_cybersecurity_incident_response.png\",\n",
    "#     \"9_cybersecurity_incident_response_v2.png\"\n",
    "# ]\n",
    "\n",
    "user_pdf = \"6_Incident_Management_V2.pdf\"\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "qwen_response_rag, retrieved_pdfs, retrieved_images = compare_query_qwen_with_rag(\n",
    "    query=user_query,\n",
    "    flowchart_imgs=flowchart_imgs,\n",
    "    user_provided_pdfs=user_pdf\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nü§ñ Qwen's Response (With RAG):\\n{qwen_response_rag}\")\n",
    "print(f\"üîç Retrieved Documents: {retrieved_pdfs}\")\n",
    "print(f\"üñºÔ∏è Retrieved Flowcharts: {retrieved_images}\")\n",
    "print(f\"‚è≥ Execution Time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5094d2e1-0daf-40fd-a084-8ab03b3b683f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26fda206-ca13-4497-9313-462900efb1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test saved documents in FAISS database\n",
    "# def view_stored_pdfs():\n",
    "#     \"\"\"Display all stored PDFs and their extracted text.\"\"\"\n",
    "#     for filename, text in pdf_texts.items():\n",
    "#         print(f\"üìÑ PDF: {filename}\\n\")\n",
    "#         print(f\"Extracted Content:\\n{text[:1000]}\")  # Show first 1000 characters\n",
    "#         print(\"=\"*80)\n",
    "\n",
    "# view_stored_pdfs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "766448a9-e470-49d0-9eff-3adef9c37473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Test saved images in FAISS database\n",
    "\n",
    "# def view_stored_flowcharts():\n",
    "#     \"\"\"Display all stored flowchart images and their filenames.\"\"\"\n",
    "#     for filename in image_filenames:\n",
    "#         image_path = os.path.join(image_dir, filename)\n",
    "#         image = Image.open(image_path)\n",
    "        \n",
    "#         plt.figure(figsize=(5, 5))\n",
    "#         plt.imshow(image)\n",
    "#         plt.axis(\"off\")\n",
    "#         plt.title(f\"üñºÔ∏è Flowchart: {filename}\")\n",
    "#         plt.show()\n",
    "\n",
    "# view_stored_flowcharts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9e5fcdf-793f-41ba-a300-ff851d41b6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_pdf_embedding(index=11):\n",
    "#     \"\"\"Check stored text embedding by retrieving the closest match for a given PDF.\"\"\"\n",
    "#     query_embedding = pdf_embeddings[index].reshape(1, -1)\n",
    "#     _, retrieved_indices = text_index.search(query_embedding, 1)\n",
    "    \n",
    "#     original_pdf = pdf_filenames[index]\n",
    "#     matched_pdf = pdf_filenames[retrieved_indices[0][0]]\n",
    "    \n",
    "#     print(f\"üìÑ Original PDF: {original_pdf}\")\n",
    "#     print(f\"üîç Closest Match: {matched_pdf}\")\n",
    "#     print(f\"Similarity Score: {np.dot(pdf_embeddings[index], pdf_embeddings[retrieved_indices[0][0]])}\")\n",
    "    \n",
    "# check_pdf_embedding()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b23561cd-0ea7-427f-8c03-41c278131caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def check_image_embedding(index=3):\n",
    "#     \"\"\"Check stored image embedding by retrieving the closest match for a given flowchart.\"\"\"\n",
    "#     query_embedding = image_embeddings[index].reshape(1, -1)\n",
    "#     _, retrieved_indices = image_index.search(query_embedding, 1)\n",
    "    \n",
    "#     original_image = image_filenames[index]\n",
    "#     matched_image = image_filenames[retrieved_indices[0][0]]\n",
    "    \n",
    "#     print(f\"üñºÔ∏è Original Flowchart: {original_image}\")\n",
    "#     print(f\"üîç Closest Match: {matched_image}\")\n",
    "#     print(f\"Similarity Score: {np.dot(image_embeddings[index], image_embeddings[retrieved_indices[0][0]])}\")\n",
    "    \n",
    "#     # Show both images\n",
    "#     fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "#     ax[0].imshow(Image.open(os.path.join(image_dir, original_image)))\n",
    "#     ax[0].set_title(\"Original Image\")\n",
    "#     ax[0].axis(\"off\")\n",
    "    \n",
    "#     ax[1].imshow(Image.open(os.path.join(image_dir, matched_image)))\n",
    "#     ax[1].set_title(\"Closest Match\")\n",
    "#     ax[1].axis(\"off\")\n",
    "    \n",
    "#     plt.show()\n",
    "\n",
    "# check_image_embedding()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ef5c1cd-0f45-4618-975d-6d379d74c79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if all image embeddings are unique\n",
    "# unique_embeddings = np.unique(image_embeddings, axis=0)\n",
    "\n",
    "# if unique_embeddings.shape[0] == 1:\n",
    "#     print(\"‚ö†Ô∏è WARNING: All image embeddings are identical! FAISS cannot differentiate them.\")\n",
    "# else:\n",
    "#     print(f\"‚úÖ FAISS has {unique_embeddings.shape[0]} unique image embeddings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "019b9bdf-1990-476b-9466-49693196db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print shapes of stored image embeddings\n",
    "# print(f\"Stored Image Embeddings Shape: {image_embeddings.shape}\")\n",
    "\n",
    "# # Generate a query embedding for comparison\n",
    "# query_embedding = get_query_image_embedding(os.path.join(image_dir, image_filenames[0]))  # Use any image as query\n",
    "# print(f\"Query Image Embedding Shape: {query_embedding.shape}\")\n",
    "\n",
    "# # Print first stored embedding vs query embedding\n",
    "# print(f\"\\nFirst Stored Embedding:\\n{image_embeddings[0][:10]}\")  # Print first 10 values\n",
    "# print(f\"\\nQuery Embedding:\\n{query_embedding[:10]}\")  # Print first 10 values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebf60dc3-dc92-4790-affa-50e372f30d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def debug_faiss_retrieval(index=3):\n",
    "#     \"\"\"Check if FAISS is properly differentiating images.\"\"\"\n",
    "#     query_embedding = image_embeddings[index].reshape(1, -1)  # Use stored image for retrieval test\n",
    "#     _, retrieved_indices = image_index.search(query_embedding, 5)  # Top 5 results\n",
    "\n",
    "#     print(f\"üñºÔ∏è Original Flowchart: {image_filenames[index]}\")\n",
    "#     print(f\"\\nüîç Closest Matches:\")\n",
    "#     for rank, idx in enumerate(retrieved_indices[0]):\n",
    "#         matched_image = image_filenames[idx]\n",
    "#         similarity_score = np.dot(image_embeddings[index], image_embeddings[idx])  # Cosine similarity\n",
    "#         print(f\"{rank + 1}. {matched_image} (Score: {similarity_score:.6f})\")\n",
    "\n",
    "# debug_faiss_retrieval()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
