{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc549157-03fd-47f4-b7ee-e1d78e3d5ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Stored document text embeddings in FAISS database.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "import fitz  \n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "### DOCUMENT PROCESSING ###\n",
    "\n",
    "# Load text embedding model\n",
    "text_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a given PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text(\"text\") + \"\\n\"\n",
    "    return text.strip()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Enhance retrieval by applying TF-IDF weighting.\"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
    "    X = vectorizer.fit_transform([text]).toarray()\n",
    "    return \" \".join(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Directory setup\n",
    "base_dir = \"business_flowcharts\"\n",
    "pdf_dir = os.path.join(base_dir, \"documents\")\n",
    "\n",
    "# Read and process PDFs\n",
    "pdf_texts = {}\n",
    "pdf_filenames = sorted([f for f in os.listdir(pdf_dir) if f.endswith(\".pdf\")])\n",
    "\n",
    "pdf_embeddings = []\n",
    "for file in pdf_filenames:\n",
    "    pdf_path = os.path.join(pdf_dir, file)\n",
    "    raw_text = extract_text_from_pdf(pdf_path)\n",
    "    processed_text = preprocess_text(raw_text)\n",
    "    pdf_texts[file] = raw_text\n",
    "    embedding = text_model.encode(processed_text)\n",
    "    pdf_embeddings.append(embedding)\n",
    "\n",
    "# Convert to FAISS-compatible format\n",
    "pdf_embeddings = np.array(pdf_embeddings, dtype=\"float32\")\n",
    "pdf_embeddings /= np.linalg.norm(pdf_embeddings, axis=1, keepdims=True)  # Normalize (IMPORTANT)\n",
    "\n",
    "# Create FAISS index\n",
    "text_index = faiss.IndexFlatL2(pdf_embeddings.shape[1])\n",
    "text_index.add(pdf_embeddings)\n",
    "\n",
    "print(\"✅ Stored document text embeddings in FAISS database.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "234472c5-6981-489f-afda-da8f1d784f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Stored flowchart image embeddings in FAISS database.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "### FLOWCHART IMG PROCESSING ###\n",
    "\n",
    "# Load CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"Preprocess an image dynamically while maintaining aspect ratio.\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Resize while keeping aspect ratio\n",
    "    aspect_ratio = image.width / image.height\n",
    "    if aspect_ratio > 1:\n",
    "        new_width = 224\n",
    "        new_height = int(224 / aspect_ratio)\n",
    "    else:\n",
    "        new_height = 224\n",
    "        new_width = int(224 * aspect_ratio)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((new_height, new_width)),  # Maintain aspect ratio\n",
    "        transforms.Pad((0, 0, 224 - new_width, 224 - new_height), fill=(255, 255, 255)),  # Pad with white\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.481, 0.457, 0.408], std=[0.268, 0.261, 0.275]),  # CLIP normalization (ALSO IMPORATNT)\n",
    "    ])\n",
    "\n",
    "    return transform(image).unsqueeze(0)\n",
    "\n",
    "def get_image_embedding(image_path):\n",
    "    \"\"\"Generate an embedding for a flowchart image using CLIP.\"\"\"\n",
    "    image_tensor = preprocess_image(image_path).to(device)\n",
    "    with torch.no_grad():\n",
    "        embedding = clip_model.encode_image(image_tensor).cpu().numpy()\n",
    "    return embedding.flatten()\n",
    "\n",
    "# Directory setup\n",
    "image_dir = os.path.join(base_dir, \"flowcharts\")\n",
    "image_filenames = sorted([f for f in os.listdir(image_dir) if f.endswith(\".png\")])\n",
    "\n",
    "# Process images\n",
    "image_embeddings = [get_image_embedding(os.path.join(image_dir, file)) for file in image_filenames]\n",
    "image_embeddings = np.array(image_embeddings, dtype=\"float32\")\n",
    "image_embeddings /= np.linalg.norm(image_embeddings, axis=1, keepdims=True)  # Normalize\n",
    "\n",
    "# Create FAISS index\n",
    "image_index = faiss.IndexFlatL2(image_embeddings.shape[1])\n",
    "image_index.add(image_embeddings)\n",
    "\n",
    "print(\"✅ Stored flowchart image embeddings in FAISS database.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31fcba5a-5490-47e1-9685-7e66bd084120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "def extract_text_from_image(image_path):\n",
    "    \"\"\"Extract text from a given flowchart image using OCR.\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    extracted_text = pytesseract.image_to_string(image)\n",
    "    return extracted_text.strip()\n",
    "\n",
    "def get_text_embedding(text):\n",
    "    \"\"\"Generate an embedding for extracted text using Sentence-BERT.\"\"\"\n",
    "    return text_model.encode(text)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2285b3dc-6fcf-4600-9620-b7540d81ec4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Retrieval function V2 is ready.\n"
     ]
    }
   ],
   "source": [
    "def get_query_image_embedding(image_path):\n",
    "    \"\"\"Generate a normalized embedding for a query image using CLIP.\"\"\"\n",
    "    image_tensor = preprocess_image(image_path).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embedding = clip_model.encode_image(image_tensor).cpu().numpy()\n",
    "    \n",
    "    return embedding.flatten() / np.linalg.norm(embedding)  # Normalize\n",
    "\n",
    "def retrieve_relevant_data(query, flowchart_imgs=None, top_k=2):\n",
    "    \"\"\"Retrieve the most relevant documents & images for the query using FAISS.\"\"\"\n",
    "\n",
    "    if flowchart_imgs and isinstance(flowchart_imgs, str):\n",
    "        flowchart_imgs = [flowchart_imgs]  # Ensure list format\n",
    "\n",
    "    # Convert query to text embedding\n",
    "    query_embedding = text_model.encode(query).reshape(1, -1)\n",
    "    query_embedding /= np.linalg.norm(query_embedding)  # Normalize\n",
    "\n",
    "    # Search FAISS text database (Retrieve relevant PDFs)\n",
    "    text_distances, text_results = text_index.search(query_embedding, top_k)\n",
    "    retrieved_pdfs = [(pdf_filenames[idx], text_distances[0][i]) for i, idx in enumerate(text_results[0])]\n",
    "\n",
    "    retrieved_images = []\n",
    "    image_scores = []\n",
    "    unknown_images = []  # Store unrecognized images\n",
    "\n",
    "    # Get a set of indexed flowcharts\n",
    "    indexed_flowcharts = set(image_filenames)  # All pre-indexed flowcharts\n",
    "\n",
    "    if flowchart_imgs:\n",
    "        for flowchart_img in flowchart_imgs:\n",
    "            # If the image is outside of the indexed directory, mark as NEW\n",
    "            if os.path.basename(flowchart_img) not in indexed_flowcharts:\n",
    "                print(f\"🆕 Marking '{flowchart_img}' as [NEW FLOWCHART] (not in FAISS index)\")\n",
    "                unknown_images.append(flowchart_img)\n",
    "                continue  # Skip FAISS retrieval\n",
    "\n",
    "            # Normal FAISS retrieval for known images\n",
    "            extracted_text = extract_text_from_image(flowchart_img)\n",
    "            text_embedding = get_text_embedding(extracted_text)\n",
    "\n",
    "            # Retrieve documents based on extracted image text\n",
    "            text_distances, text_results = text_index.search(text_embedding.reshape(1, -1), top_k)\n",
    "            retrieved_pdfs += [(pdf_filenames[idx], text_distances[0][i]) for i, idx in enumerate(text_results[0])]\n",
    "\n",
    "            # Get CLIP image embedding\n",
    "            image_query_embedding = get_query_image_embedding(flowchart_img)\n",
    "\n",
    "            # Retrieve similar images from FAISS (with distance scores)\n",
    "            image_distances, image_results = image_index.search(image_query_embedding.reshape(1, -1), top_k)\n",
    "\n",
    "            if image_results[0][0] >= 0:  # If valid results exist\n",
    "                retrieved_images += [image_filenames[idx] for idx in image_results[0]]\n",
    "                image_scores += list(image_distances[0])  # Store distances\n",
    "            else:\n",
    "                unknown_images.append(flowchart_img)  # Mark as unknown\n",
    "\n",
    "    # Sort PDFs & images by FAISS similarity scores\n",
    "    retrieved_pdfs = sorted(set(retrieved_pdfs), key=lambda x: x[1])[:top_k]\n",
    "    retrieved_pdfs = [pdf for pdf, _ in retrieved_pdfs]  # Keep only filenames\n",
    "\n",
    "    # Sort images by similarity scores\n",
    "    image_sorted = sorted(zip(retrieved_images, image_scores), key=lambda x: x[1])[:top_k]\n",
    "    retrieved_images = [img for img, _ in image_sorted]  # Extract sorted filenames\n",
    "\n",
    "    # Append unknown images explicitly labeled as \"New Flowchart (V2)\"\n",
    "    retrieved_images += [f\"[NEW FLOWCHART] {img}\" for img in unknown_images]\n",
    "\n",
    "    return retrieved_pdfs, retrieved_images\n",
    "\n",
    "\n",
    "print(\"✅ Retrieval function V2 is ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd22c25b-b1a2-459b-926b-60adfc08d7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfdf71dce70446acac206f4e3e592f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n",
    "\n",
    "# Load Qwen model\n",
    "qwen_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\", \n",
    "    torch_dtype=torch.bfloat16,  \n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "qwen_processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "031ac69c-2195-4f38-86df-414c6e47d450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def debug_query_qwen_with_rag(query, flowchart_img=None, top_k=2):\n",
    "#     \"\"\"DEBUG MODE - Retrieve relevant flowchart data (text & images) and query Qwen for an AI-generated response.\"\"\"\n",
    "    \n",
    "#     # Retrieve relevant PDFs (text) & Flowcharts (images)\n",
    "#     retrieved_pdfs, retrieved_images = retrieve_relevant_data(query, flowchart_img, top_k)\n",
    "    \n",
    "#     # Extract text from retrieved PDFs\n",
    "#     context = \"\\n\".join([pdf_texts[pdf] for pdf in retrieved_pdfs])\n",
    "\n",
    "#     # Use retrieved flowchart image if `flowchart_img` is None\n",
    "#     image_path = flowchart_img if flowchart_img else os.path.join(image_dir, retrieved_images[0])\n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "#     # Define the user message (Injecting retrieved context)\n",
    "#     messages = [\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": [\n",
    "#                 {\"type\": \"image\", \"image\": image},\n",
    "#                 {\"type\": \"text\", \"text\": f\"Attached is document providing context. Based on the provided image and textual information, please analyze the content and generate a response that accurately addresses the user's inquiry.\\n\\nContext:\\n{context}\\n\\nQuery: {query}\"},\n",
    "#             ],\n",
    "#         }\n",
    "#     ]\n",
    "    \n",
    "#     # Format input for Qwen\n",
    "#     text = qwen_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "#     inputs = qwen_processor(\n",
    "#         text=[text],\n",
    "#         images=[image],\n",
    "#         padding=True,\n",
    "#         return_tensors=\"pt\",\n",
    "#     ).to(qwen_model.device)\n",
    "    \n",
    "#     # Generate response\n",
    "#     with torch.no_grad():\n",
    "#         output_ids = qwen_model.generate(inputs, max_new_tokens=512)\n",
    "    \n",
    "#     # Decode response\n",
    "#     response_text = \"========\\n\\n\".join(qwen_processor.batch_decode(output_ids, skip_special_tokens=False))\n",
    "    \n",
    "#     return response_text, retrieved_pdfs, retrieved_images\n",
    "\n",
    "# print(\"✅ DEBUG Qwen RAG system is ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cee3aa1f-4093-4a85-bf0b-08e159d6f64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Qwen RAG V2 system is ready.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def query_qwen_with_rag(query, flowchart_imgs=None, top_k=2):\n",
    "    \"\"\"Retrieve relevant flowchart data (text & images) and query Qwen for an AI-generated response.\"\"\"\n",
    "\n",
    "    if flowchart_imgs and isinstance(flowchart_imgs, str):\n",
    "        flowchart_imgs = [flowchart_imgs]  # Ensure list format\n",
    "\n",
    "    # Retrieve relevant PDFs (text) & Flowcharts (images)\n",
    "    retrieved_pdfs, retrieved_images = retrieve_relevant_data(query, flowchart_imgs, top_k)\n",
    "    \n",
    "    # Extract text from retrieved PDFs\n",
    "    context = \"\\n\".join([pdf_texts[pdf] for pdf in retrieved_pdfs])\n",
    "\n",
    "    # Extract text from each flowchart individually\n",
    "    flowchart_texts = []\n",
    "    descriptions = []\n",
    "    images = []  # Store valid images\n",
    "\n",
    "    if flowchart_imgs:\n",
    "        for img_path in flowchart_imgs:\n",
    "            if img_path.startswith(\"[NEW FLOWCHART]\"):\n",
    "                descriptions.append(f\"⚠️ This is a newly provided flowchart: {img_path.replace('[NEW FLOWCHART] ', '')}\")\n",
    "                flowchart_texts.append(f\"Flowchart {len(flowchart_texts) + 1} (User-Provided Flowchart):\\n\\n[Unable to extract full text, refer to image]\")\n",
    "                continue\n",
    "\n",
    "            if not os.path.exists(img_path):\n",
    "                print(f\"⚠️ Warning: Image '{img_path}' not found. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Extract OCR text\n",
    "            extracted_text = extract_text_from_image(img_path)\n",
    "            flowchart_texts.append(f\"Flowchart {len(flowchart_texts) + 1}:\\n\\n{extracted_text}\")\n",
    "\n",
    "            # Load image\n",
    "            images.append(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "    # Ensure the AI processes both flowcharts separately\n",
    "    flowchart_section = \"\\n\\n\".join(flowchart_texts)\n",
    "\n",
    "    # Build user message content\n",
    "    content = [\n",
    "        *([{\"type\": \"image\", \"image\": img} for img in images]),  # Attach images (only if available)\n",
    "        *([{\"type\": \"text\", \"text\": desc} for desc in descriptions]),  # Describe unknown images\n",
    "        {\"type\": \"text\", \"text\": f\"Context:\\n{context}\\n\\n{flowchart_section}\\n\\n{query}\"}\n",
    "    ]\n",
    "\n",
    "    # Ensure `inputs` is correctly formatted\n",
    "    if not images:\n",
    "        print(\"🔹 No images detected, processing as a pure text query.\")\n",
    "        text_input = qwen_processor.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": query}]}], \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        inputs = qwen_processor(\n",
    "            text=[text_input],  # Ensure list format\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(qwen_model.device)\n",
    "    else:\n",
    "        text_input = qwen_processor.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": content}], \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        inputs = qwen_processor(\n",
    "            text=[text_input],  # Ensure list format\n",
    "            images=[img for img in images],  # Properly handle images\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(qwen_model.device)\n",
    "\n",
    "    # Check if inputs is correctly formatted before generating\n",
    "    if not hasattr(inputs, \"input_ids\"):\n",
    "        print(\"⚠️ Error: Inputs are incorrectly formatted. Skipping generation.\")\n",
    "        return \"Error: Invalid input formatting\", retrieved_pdfs, retrieved_images\n",
    "\n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        output_ids = qwen_model.generate(**inputs, max_new_tokens=1024)\n",
    "\n",
    "    # Decode response\n",
    "    response_text = qwen_processor.batch_decode(output_ids, skip_special_tokens=False)[0]\n",
    "\n",
    "    # Extract Assistant's Response Only\n",
    "    match = re.search(r\"assistant\\s*\\n(.*)\", response_text, re.DOTALL)\n",
    "    cleaned_response = match.group(1).strip() if match else response_text.strip()\n",
    "\n",
    "    return cleaned_response, retrieved_pdfs, retrieved_images\n",
    "\n",
    "\n",
    "print(\"✅ Qwen RAG V2 system is ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ae4f055-8f6e-48fb-b117-c4ae3962c608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤖 Qwen's Response (With RAG):\n",
      "Yes, I can see the flowchart for the Medical Diagnosis Workflow. Here are the steps listed in the flowchart along with explanations:\n",
      "\n",
      "1. **Start**\n",
      "   - The process begins when a patient presents with symptoms.\n",
      "\n",
      "2. **Patient Describes Symptoms**\n",
      "   - The patient provides a detailed account of their symptoms and medical history.\n",
      "\n",
      "3. **Conduct Medical Examination**\n",
      "   - A physician performs a physical assessment to check for visible signs of illness.\n",
      "\n",
      "4. **Order Diagnostic Tests**\n",
      "   - If necessary, additional tests such as lab tests, imaging scans, or specialized procedures are ordered.\n",
      "\n",
      "5. **Review Test Results**\n",
      "   - The physician evaluates the diagnostic reports to identify possible conditions.\n",
      "\n",
      "6. **Confirm Diagnosis**\n",
      "   - A final diagnosis is determined based on symptoms, examination findings, and test results.\n",
      "\n",
      "7. **Recommend Treatment**\n",
      "   - The appropriate treatment plan, including medication, therapy, or further medical procedures, is prescribed.\n",
      "\n",
      "8. **Diagnosis Completed**\n",
      "   - The patient receives medical guidance based on the diagnosis, and further monitoring is scheduled if needed.\n",
      "\n",
      "This flowchart outlines the systematic approach to diagnosing a patient's condition, ensuring that each step is followed logically to arrive at a definitive diagnosis and treatment plan.<|im_end|>\n",
      "🔍 Retrieved Documents: ['17_Medical_Diagnosis.pdf', '20_Emergency_Triage.pdf']\n",
      "🖼️ Retrieved Flowcharts: ['17_medical_diagnosis.png', '16_patient_admission.png']\n",
      "⏳ Execution Time: 20.60 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Define the test query and flowchart image\n",
    "user_query = \"Can you see what flowchart this is? Can you list out the steps and give an explanation?\"\n",
    "# flowchart_img = \"business_flowcharts/flowcharts/14_quality_control.png\"\n",
    "flowchart_img = \"business_flowcharts/flowcharts/17_medical_diagnosis.png\"\n",
    "# flowchart_img = \"business_flowcharts/flowcharts/24_thesis_submission.png\"\n",
    "# In this cybersecurity flowchart, explain the difference in action should the threat be labelled as critical or not.\n",
    "# Measure execution time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Query Qwen with RAG\n",
    "qwen_response_rag, retrieved_pdfs, retrieved_images = query_qwen_with_rag(user_query, flowchart_img)\n",
    "\n",
    "# Calculate total time taken\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n🤖 Qwen's Response (With RAG):\\n{qwen_response_rag}\")\n",
    "print(f\"🔍 Retrieved Documents: {retrieved_pdfs}\")\n",
    "print(f\"🖼️ Retrieved Flowcharts: {retrieved_images}\")\n",
    "print(f\"⏳ Execution Time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e241e63-d894-4d59-839f-05e1aaf35d19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bee64c3f-82d7-479a-aa95-a604f4d06471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison RAG Done!\n"
     ]
    }
   ],
   "source": [
    "def compare_query_qwen_with_rag(query, flowchart_imgs=None, top_k=2):\n",
    "    \"\"\"Retrieve relevant flowchart data (text & images) and query Qwen for an AI-generated response.\"\"\"\n",
    "\n",
    "    if flowchart_imgs and isinstance(flowchart_imgs, str):\n",
    "        flowchart_imgs = [flowchart_imgs]  # Ensure list format\n",
    "\n",
    "    # Retrieve relevant PDFs (text) & Flowcharts (images)\n",
    "    retrieved_pdfs, retrieved_images = retrieve_relevant_data(query, flowchart_imgs, top_k)\n",
    "    \n",
    "    # Extract text from retrieved PDFs\n",
    "    context = \"\\n\".join([pdf_texts[pdf] for pdf in retrieved_pdfs])\n",
    "\n",
    "    # Extract text from each flowchart individually\n",
    "    flowchart_texts = []\n",
    "    descriptions = []\n",
    "\n",
    "    if flowchart_imgs:\n",
    "        for img_path in flowchart_imgs:\n",
    "            if img_path.startswith(\"[NEW FLOWCHART]\"):\n",
    "                descriptions.append(f\"⚠️ This is a newly provided flowchart: {img_path.replace('[NEW FLOWCHART] ', '')}\")\n",
    "                flowchart_texts.append(f\"Flowchart {len(flowchart_texts) + 1} (User-Provided Flowchart):\\n\\n[Unable to extract full text, refer to image]\")  # Placeholder for new flowchart\n",
    "                continue\n",
    "\n",
    "            if not os.path.exists(img_path):\n",
    "                print(f\"⚠️ Warning: Image '{img_path}' not found. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Extract OCR text\n",
    "            extracted_text = extract_text_from_image(img_path)\n",
    "\n",
    "            # Ensure flowchart text is formatted properly\n",
    "            flowchart_texts.append(f\"Flowchart {len(flowchart_texts) + 1}:\\n\\n{extracted_text}\")\n",
    "\n",
    "    # Ensure the AI processes both flowcharts separately\n",
    "    flowchart_section = \"\\n\\n\".join(flowchart_texts)\n",
    "\n",
    "    # Define the user message (Injecting retrieved context)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                *([{\"type\": \"image\", \"image\": Image.open(img).convert(\"RGB\")} for img in flowchart_imgs if not img.startswith(\"[NEW FLOWCHART]\")]),  # Attach images\n",
    "                *([{\"type\": \"text\", \"text\": desc} for desc in descriptions]),  # Debug: new user inserted images\n",
    "                {\"type\": \"text\", \"text\": f\"Below are the details extracted from the flowcharts provided:\\n\\n{flowchart_section}\\n\\nCompare the flowcharts carefully. Provide step-by-step differences and explain any structural changes.\\n\\n{query}\"},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Format input for Qwen\n",
    "    text = qwen_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = qwen_processor(\n",
    "        text=[text],\n",
    "        images=[Image.open(img).convert(\"RGB\") for img in flowchart_imgs if not img.startswith(\"[NEW FLOWCHART]\")],  # Provide images\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(qwen_model.device)\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        output_ids = qwen_model.generate(**inputs, max_new_tokens=1024)  # Increased token limit\n",
    "\n",
    "    # Decode response\n",
    "    response_text = qwen_processor.batch_decode(output_ids, skip_special_tokens=False)[0]\n",
    "\n",
    "    # Extract Assistant's Response Only\n",
    "    match = re.search(r\"assistant\\s*\\n(.*)\", response_text, re.DOTALL)\n",
    "    cleaned_response = match.group(1).strip() if match else response_text.strip()\n",
    "\n",
    "    return cleaned_response, retrieved_pdfs, retrieved_images\n",
    "\n",
    "print(\"Comparison RAG Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "842614af-fc72-404b-8bbe-072c7bc1db2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🆕 Marking '9_cybersecurity_incident_response_v2.png' as [NEW FLOWCHART] (not in FAISS index)\n",
      "\n",
      "🤖 Qwen's Response (With RAG):\n",
      "Certainly! Let's compare the two flowcharts step by step, highlighting the differences and explaining any structural changes.\n",
      "\n",
      "### Flowchart 1:\n",
      "**Cybersecurity Incident Response**\n",
      "\n",
      "1. **Start**\n",
      "2. **Detect Security Threat**\n",
      "3. **Analyze Threat**\n",
      "4. **Critical Threat?**\n",
      "   - If **Yes**: Deploy Security Patch\n",
      "   - If **No**: Apply Mitigation\n",
      "5. **Monitor System**\n",
      "6. **Incident Resolved**\n",
      "\n",
      "### Flowchart 2:\n",
      "**Cybersecurity Incident Response (Upgraded)**\n",
      "\n",
      "1. **Start**\n",
      "2. **Detect Security Threat**\n",
      "3. **Classify Threat**\n",
      "4. **Analyze Threat Impact**\n",
      "5. **Critical Threat?**\n",
      "   - If **Yes**: Escalate to Security Team\n",
      "   - If **No**: Apply Mitigation Measures\n",
      "6. **Deploy Security Patch**\n",
      "7. **Monitor System for Anomalies**\n",
      "8. **Post-Incident Review & Reporting**\n",
      "9. **Incident Resolved**\n",
      "\n",
      "### Differences:\n",
      "\n",
      "1. **Step 2: Detect Security Threat**\n",
      "   - **Flowchart 1**: Directly moves to \"Analyze Threat.\"\n",
      "   - **Flowchart 2**: Moves to \"Classify Threat\" before \"Analyze Threat Impact.\"\n",
      "\n",
      "2. **Step 5: Critical Threat?**\n",
      "   - **Flowchart 1**: Directly branches into \"Apply Mitigation\" or \"Deploy Security Patch.\"\n",
      "   - **Flowchart 2**: Branches into \"Apply Mitigation Measures\" if not critical threat.\n",
      "\n",
      "3. **Step 6: Monitor System**\n",
      "   - **Flowchart 1**: Directly follows \"Apply Mitigation\" or \"Deploy Security Patch.\"\n",
      "   - **Flowchart 2**: Follows \"Deploy Security Patch\" and then \"Monitor System for Anomalies.\"\n",
      "\n",
      "4. **Step 8: Post-Incident Review & Reporting**\n",
      "   - **Flowchart 1**: No specific step for post-incident review.\n",
      "   - **Flowchart 2**: Added this step after \"Monitor System for Anomalies.\"\n",
      "\n",
      "5. **Step 9: Incident Resolved**\n",
      "   - **Flowchart 1**: Ends with \"Incident Resolved.\"\n",
      "   - **Flowchart 2**: Ends with \"Incident Resolved,\" but includes an additional step of \"Post-Incident Review & Reporting.\"\n",
      "\n",
      "### Summary of Changes:\n",
      "\n",
      "1. **Classification of Threat**: The second flowchart introduces a classification step (\"Classify Threat\") before analyzing the impact of the threat, which is not present in the first flowchart.\n",
      "2. **Mitigation Measures**: The second flowchart specifies that mitigation measures should be applied if the threat is not critical, whereas the first flowchart directly branches into mitigation without specifying conditions.\n",
      "3. **Monitoring System**: The second flowchart adds a step to monitor the system for anomalies after deploying security patches, which is not present in the first flowchart.\n",
      "4. **Post-Incident Review**: The second flowchart includes a post-incident review step, which is absent in the first flowchart.\n",
      "\n",
      "These changes enhance the detailed handling of cybersecurity incidents by adding steps for classification, monitoring, and post-incident review, providing a more comprehensive response process.<|im_end|>\n",
      "🔍 Retrieved Documents: ['9_Cybersecurity_Incident_Response.pdf', '6_Incident_Management.pdf']\n",
      "🖼️ Retrieved Flowcharts: ['9_cybersecurity_incident_response.png', '6_incident_management.png', '[NEW FLOWCHART] 9_cybersecurity_incident_response_v2.png']\n",
      "⏳ Execution Time: 55.72 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Define the test query and multiple flowchart images\n",
    "# user_query = \"Can you compare these flowcharts in detail? What are the difference between the first and the second one. I'm working on the second one to give more detail.\"\n",
    "user_query = \"Can you compare these flowcharts in detail? What are the difference between the first and the second one. I'm working on the second one to give more detail. Summarize each comparison.\"\n",
    "# flowchart_imgs = [\n",
    "#     \"business_flowcharts/flowcharts/6_incident_management.png\",\n",
    "#     \"6_incident_management_v2.png\"\n",
    "# ]\n",
    "\n",
    "flowchart_imgs = [\n",
    "    \"business_flowcharts/flowcharts/9_cybersecurity_incident_response.png\",\n",
    "    \"9_cybersecurity_incident_response_v2.png\"\n",
    "]\n",
    "# Measure execution time\n",
    "start_time = time.time()\n",
    "\n",
    "# Query Qwen with RAG\n",
    "qwen_response_rag, retrieved_pdfs, retrieved_images = compare_query_qwen_with_rag(user_query, flowchart_imgs)\n",
    "\n",
    "# Calculate total time taken\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n🤖 Qwen's Response (With RAG):\\n{qwen_response_rag}\")\n",
    "print(f\"🔍 Retrieved Documents: {retrieved_pdfs}\")\n",
    "print(f\"🖼️ Retrieved Flowcharts: {retrieved_images}\")\n",
    "print(f\"⏳ Execution Time: {execution_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5094d2e1-0daf-40fd-a084-8ab03b3b683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# # STILL WIP - DO NOT USE THIS YET, STILL TESTING IT.\n",
    "\n",
    "# # Initialize memory store\n",
    "# conversation_memory = []\n",
    "\n",
    "# def add_to_memory(user_query, qwen_response, max_memory=5):\n",
    "#     \"\"\"Stores past interactions while keeping memory manageable.\"\"\"\n",
    "#     conversation_memory.append({\"query\": user_query, \"response\": qwen_response})\n",
    "\n",
    "#     # Keep only the last 'max_memory' interactions\n",
    "#     if len(conversation_memory) > max_memory:\n",
    "#         conversation_memory.pop(0)  # Remove the oldest memory\n",
    "\n",
    "\n",
    "# def memory_query_qwen_with_rag(query, flowchart_imgs=None, top_k=2):\n",
    "#     \"\"\"Retrieve relevant flowchart data (text & images) and query Qwen with memory support.\"\"\"\n",
    "\n",
    "#     if flowchart_imgs and isinstance(flowchart_imgs, str):\n",
    "#         flowchart_imgs = [flowchart_imgs]  # Ensure list format\n",
    "\n",
    "#     # Retrieve relevant PDFs (text) & Flowcharts (images)\n",
    "#     retrieved_pdfs, retrieved_images = retrieve_relevant_data(query, flowchart_imgs, top_k)\n",
    "    \n",
    "#     # Extract text from retrieved PDFs\n",
    "#     context = \"\\n\".join([pdf_texts[pdf] for pdf in retrieved_pdfs])\n",
    "\n",
    "#     # Construct memory context (only if memory exists)\n",
    "#     memory_context = \"\\n\\n\".join(\n",
    "#         [f\"User: {entry['query']}\\nAssistant: {entry['response']}\" for entry in conversation_memory]\n",
    "#     )\n",
    "    \n",
    "#     # Combine memory with the new query\n",
    "#     full_query = f\"Context:\\n{memory_context}\\n\\nUser's new question:\\n{query}\" if memory_context else query\n",
    "\n",
    "#     # Extract text from each flowchart individually\n",
    "#     flowchart_texts = []\n",
    "#     descriptions = []\n",
    "#     images = []  # Store valid images\n",
    "\n",
    "#     if flowchart_imgs:\n",
    "#         for img_path in flowchart_imgs:\n",
    "#             if img_path.startswith(\"[NEW FLOWCHART]\"):\n",
    "#                 descriptions.append(f\"⚠️ This is a newly provided flowchart: {img_path.replace('[NEW FLOWCHART] ', '')}\")\n",
    "#                 flowchart_texts.append(f\"Flowchart {len(flowchart_texts) + 1} (User-Provided Flowchart):\\n\\n[Unable to extract full text, refer to image]\")  # Placeholder for new flowchart\n",
    "#                 continue\n",
    "\n",
    "#             if not os.path.exists(img_path):\n",
    "#                 print(f\"⚠️ Warning: Image '{img_path}' not found. Skipping.\")\n",
    "#                 continue\n",
    "\n",
    "#             # Extract OCR text\n",
    "#             extracted_text = extract_text_from_image(img_path)\n",
    "#             flowchart_texts.append(f\"Flowchart {len(flowchart_texts) + 1}:\\n\\n{extracted_text}\")\n",
    "\n",
    "#             # Load image\n",
    "#             images.append(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "#     # Ensure the AI processes both flowcharts separately\n",
    "#     flowchart_section = \"\\n\\n\".join(flowchart_texts)\n",
    "\n",
    "#     # Build user message content\n",
    "#     content = [\n",
    "#         *([{\"type\": \"image\", \"image\": img} for img in images]),  # Attach images - only if available\n",
    "#         *([{\"type\": \"text\", \"text\": desc} for desc in descriptions]),  # Describe unknown images\n",
    "#         {\"type\": \"text\", \"text\": f\"Context:\\n{context}\\n\\n{flowchart_section}\\n\\n{full_query}\"}\n",
    "#     ]\n",
    "\n",
    "#     # **Ensure inputs are always properly formatted**\n",
    "#     if not images:  \n",
    "#         print(\"🔹 No images detected, processing as a pure text query.\")\n",
    "#         text_input = qwen_processor.apply_chat_template(\n",
    "#             [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": full_query}]}], \n",
    "#             tokenize=False, \n",
    "#             add_generation_prompt=True\n",
    "#         )\n",
    "#         inputs = qwen_processor(\n",
    "#             text=[text_input],  # **Ensure list format**\n",
    "#             return_tensors=\"pt\"\n",
    "#         ).to(qwen_model.device)\n",
    "#     else:\n",
    "#         text_input = qwen_processor.apply_chat_template(\n",
    "#             [{\"role\": \"user\", \"content\": content}], \n",
    "#             tokenize=False, \n",
    "#             add_generation_prompt=True\n",
    "#         )\n",
    "#         inputs = qwen_processor(\n",
    "#             text=[text_input],  # **Ensure list format**\n",
    "#             images=images,\n",
    "#             padding=True,\n",
    "#             return_tensors=\"pt\"\n",
    "#         ).to(qwen_model.device)\n",
    "\n",
    "#     # **Check if inputs is properly formatted before generating**\n",
    "#     if not inputs or \"input_ids\" not in inputs:\n",
    "#         print(\"⚠️ Error: Inputs are incorrectly formatted. Skipping generation.\")\n",
    "#         return \"Error: Invalid input formatting\", retrieved_pdfs, retrieved_images\n",
    "\n",
    "#     # Generate response\n",
    "#     with torch.no_grad():\n",
    "#         output_ids = qwen_model.generate(**inputs, max_new_tokens=1024)\n",
    "\n",
    "#     # Decode response\n",
    "#     response_text = qwen_processor.batch_decode(output_ids, skip_special_tokens=False)[0]\n",
    "\n",
    "#     # Extract Assistant's Response Only\n",
    "#     match = re.search(r\"assistant\\s*\\n(.*)\", response_text, re.DOTALL)\n",
    "#     cleaned_response = match.group(1).strip() if match else response_text.strip()\n",
    "\n",
    "#     # Store the new interaction in memory\n",
    "#     add_to_memory(query, cleaned_response)\n",
    "\n",
    "#     return cleaned_response, retrieved_pdfs, retrieved_images\n",
    "\n",
    "\n",
    "# print(\"✅ Qwen RAG V2 system with Memory is ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5eaec6f0-00f1-4076-8247-ddb65be869c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First query\n",
    "# response_1, _, _ = memory_query_qwen_with_rag(\"What is the process in the incident management flowchart?\")\n",
    "# print(\"\\n🔹 First Response:\", response_1)\n",
    "\n",
    "# # Second query (without re-explaining)\n",
    "# response_2, _, _ = memory_query_qwen_with_rag(\"Can you explain how it escalates issues?\")\n",
    "# print(\"\\n🔹 Second Response:\", response_2)\n",
    "\n",
    "# # Summary\n",
    "# response_2, _, _ = memory_query_qwen_with_rag(\"What have we discussed so far?\")\n",
    "# print(\"\\n🔹 Second Response:\", response_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "185b9dfb-7ec5-43c1-8aab-a365f5a28fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Different cell now\n",
    "# response_2, _, _ = memory_query_qwen_with_rag(\"You remember what we have discussed so far?\")\n",
    "# print(\"\\n🔹 Second Response:\", response_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c8f06d-17ad-4b7d-a75b-ea83294d0c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f007b3e-16a7-453f-8c33-337db340f590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec89d59-15df-4179-8a3a-697a9e00ce6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26fda206-ca13-4497-9313-462900efb1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test saved documents in FAISS database\n",
    "# def view_stored_pdfs():\n",
    "#     \"\"\"Display all stored PDFs and their extracted text.\"\"\"\n",
    "#     for filename, text in pdf_texts.items():\n",
    "#         print(f\"📄 PDF: {filename}\\n\")\n",
    "#         print(f\"Extracted Content:\\n{text[:1000]}\")  # Show first 1000 characters\n",
    "#         print(\"=\"*80)\n",
    "\n",
    "# view_stored_pdfs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "766448a9-e470-49d0-9eff-3adef9c37473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Test saved images in FAISS database\n",
    "\n",
    "# def view_stored_flowcharts():\n",
    "#     \"\"\"Display all stored flowchart images and their filenames.\"\"\"\n",
    "#     for filename in image_filenames:\n",
    "#         image_path = os.path.join(image_dir, filename)\n",
    "#         image = Image.open(image_path)\n",
    "        \n",
    "#         plt.figure(figsize=(5, 5))\n",
    "#         plt.imshow(image)\n",
    "#         plt.axis(\"off\")\n",
    "#         plt.title(f\"🖼️ Flowchart: {filename}\")\n",
    "#         plt.show()\n",
    "\n",
    "# view_stored_flowcharts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9e5fcdf-793f-41ba-a300-ff851d41b6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_pdf_embedding(index=11):\n",
    "#     \"\"\"Check stored text embedding by retrieving the closest match for a given PDF.\"\"\"\n",
    "#     query_embedding = pdf_embeddings[index].reshape(1, -1)\n",
    "#     _, retrieved_indices = text_index.search(query_embedding, 1)\n",
    "    \n",
    "#     original_pdf = pdf_filenames[index]\n",
    "#     matched_pdf = pdf_filenames[retrieved_indices[0][0]]\n",
    "    \n",
    "#     print(f\"📄 Original PDF: {original_pdf}\")\n",
    "#     print(f\"🔍 Closest Match: {matched_pdf}\")\n",
    "#     print(f\"Similarity Score: {np.dot(pdf_embeddings[index], pdf_embeddings[retrieved_indices[0][0]])}\")\n",
    "    \n",
    "# check_pdf_embedding()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b23561cd-0ea7-427f-8c03-41c278131caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def check_image_embedding(index=3):\n",
    "#     \"\"\"Check stored image embedding by retrieving the closest match for a given flowchart.\"\"\"\n",
    "#     query_embedding = image_embeddings[index].reshape(1, -1)\n",
    "#     _, retrieved_indices = image_index.search(query_embedding, 1)\n",
    "    \n",
    "#     original_image = image_filenames[index]\n",
    "#     matched_image = image_filenames[retrieved_indices[0][0]]\n",
    "    \n",
    "#     print(f\"🖼️ Original Flowchart: {original_image}\")\n",
    "#     print(f\"🔍 Closest Match: {matched_image}\")\n",
    "#     print(f\"Similarity Score: {np.dot(image_embeddings[index], image_embeddings[retrieved_indices[0][0]])}\")\n",
    "    \n",
    "#     # Show both images\n",
    "#     fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "#     ax[0].imshow(Image.open(os.path.join(image_dir, original_image)))\n",
    "#     ax[0].set_title(\"Original Image\")\n",
    "#     ax[0].axis(\"off\")\n",
    "    \n",
    "#     ax[1].imshow(Image.open(os.path.join(image_dir, matched_image)))\n",
    "#     ax[1].set_title(\"Closest Match\")\n",
    "#     ax[1].axis(\"off\")\n",
    "    \n",
    "#     plt.show()\n",
    "\n",
    "# check_image_embedding()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ef5c1cd-0f45-4618-975d-6d379d74c79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if all image embeddings are unique\n",
    "# unique_embeddings = np.unique(image_embeddings, axis=0)\n",
    "\n",
    "# if unique_embeddings.shape[0] == 1:\n",
    "#     print(\"⚠️ WARNING: All image embeddings are identical! FAISS cannot differentiate them.\")\n",
    "# else:\n",
    "#     print(f\"✅ FAISS has {unique_embeddings.shape[0]} unique image embeddings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "019b9bdf-1990-476b-9466-49693196db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print shapes of stored image embeddings\n",
    "# print(f\"Stored Image Embeddings Shape: {image_embeddings.shape}\")\n",
    "\n",
    "# # Generate a query embedding for comparison\n",
    "# query_embedding = get_query_image_embedding(os.path.join(image_dir, image_filenames[0]))  # Use any image as query\n",
    "# print(f\"Query Image Embedding Shape: {query_embedding.shape}\")\n",
    "\n",
    "# # Print first stored embedding vs query embedding\n",
    "# print(f\"\\nFirst Stored Embedding:\\n{image_embeddings[0][:10]}\")  # Print first 10 values\n",
    "# print(f\"\\nQuery Embedding:\\n{query_embedding[:10]}\")  # Print first 10 values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebf60dc3-dc92-4790-affa-50e372f30d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def debug_faiss_retrieval(index=3):\n",
    "#     \"\"\"Check if FAISS is properly differentiating images.\"\"\"\n",
    "#     query_embedding = image_embeddings[index].reshape(1, -1)  # Use stored image for retrieval test\n",
    "#     _, retrieved_indices = image_index.search(query_embedding, 5)  # Top 5 results\n",
    "\n",
    "#     print(f\"🖼️ Original Flowchart: {image_filenames[index]}\")\n",
    "#     print(f\"\\n🔍 Closest Matches:\")\n",
    "#     for rank, idx in enumerate(retrieved_indices[0]):\n",
    "#         matched_image = image_filenames[idx]\n",
    "#         similarity_score = np.dot(image_embeddings[index], image_embeddings[idx])  # Cosine similarity\n",
    "#         print(f\"{rank + 1}. {matched_image} (Score: {similarity_score:.6f})\")\n",
    "\n",
    "# debug_faiss_retrieval()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
